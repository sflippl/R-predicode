@article{Apps2014,
abstract = {Recognising and representing one's self as distinct from others is a fundamental component of self-awareness. However, current theories of self-recognition are not embedded within global theories of cortical function and therefore fail to provide a compelling explanation of how the self is processed. We present a theoretical account of the neural and computational basis of self-recognition that is embedded within the free-energy account of cortical function. In this account one's body is processed in a Bayesian manner as the most likely to be “me”. Such probabilistic representation arises through the integration of information from hierarchically organised unimodal systems in higher-level multimodal areas. This information takes the form of bottom-up “surprise” signals from unimodal sensory systems that are explained away by top-down processes that minimise the level of surprise across the brain. We present evidence that this theoretical perspective may account for the findings of psychological and neuroimaging investigations into self-recognition and particularly evidence that representations of the self are malleable, rather than fixed as previous accounts of self-recognition might suggest.},
author = {Apps, Matthew A.J. and Tsakiris, Manos},
doi = {10.1016/J.NEUBIOREV.2013.01.029},
file = {:home/sflippl/Documents/Literature/Papers/Apps, Tsakiris - 2014 - The free-energy self A predictive coding account of self-recognition.pdf:pdf},
issn = {0149-7634},
journal = {Neuroscience {\&} Biobehavioral Reviews},
month = {apr},
pages = {85--97},
publisher = {Pergamon},
title = {{The free-energy self: A predictive coding account of self-recognition}},
url = {https://www.sciencedirect.com/science/article/pii/S0149763413000420},
volume = {41},
year = {2014}
}
@article{Hohwy2008,
abstract = {Binocular rivalry occurs when the eyes are presented with different stimuli and subjective perception alternates between them. Though recent years have seen a number of models of this phenomenon, the mechanisms behind binocular rivalry are still debated and we still lack a principled understanding of why a cognitive system such as the brain should exhibit this striking kind of behaviour. Furthermore, psychophysical and neurophysiological (single cell and imaging) studies of rivalry are not unequivocal and have proven difficult to reconcile within one framework. This review takes an epistemological approach to rivalry that considers the brain as engaged in probabilistic unconscious perceptual inference about the causes of its sensory input. We describe a simple empirical Bayesian framework, implemented with predictive coding, which seems capable of explaining binocular rivalry and reconciling many findings. The core of the explanation is that selection of one stimulus, and subsequent alternation between stimuli in rivalry occur when: (i) there is no single model or hypothesis about the causes in the environment that enjoys both high likelihood and high prior probability and (ii) when one stimulus dominates, the bottom–up, driving signal for that stimulus is explained away while, crucially, the bottom–up signal for the suppressed stimulus is not, and remains as an unexplained but explainable prediction error signal. This induces instability in perceptual dynamics that can give rise to perceptual transitions or alternations during rivalry.},
author = {Hohwy, Jakob and Roepstorff, Andreas and Friston, Karl},
doi = {10.1016/J.COGNITION.2008.05.010},
file = {::},
issn = {0010-0277},
journal = {Cognition},
month = {sep},
number = {3},
pages = {687--701},
publisher = {Elsevier},
title = {{Predictive coding explains binocular rivalry: An epistemological review}},
url = {https://www.sciencedirect.com/science/article/pii/S0010027708001327},
volume = {108},
year = {2008}
}
@article{Kilner2007,
author = {Kilner, James M. and Friston, Karl J. and Frith, Chris D.},
doi = {10.1007/s10339-007-0170-2},
file = {::},
issn = {1612-4790},
journal = {Cognitive Processing},
month = {sep},
number = {3},
pages = {159--166},
publisher = {Springer-Verlag},
title = {{Predictive coding: an account of the mirror neuron system}},
url = {http://link.springer.com/10.1007/s10339-007-0170-2},
volume = {8},
year = {2007}
}
@article{Brodski2015,
abstract = {Recent neurophysiological accounts of predictive coding hypothesized that a mismatch of prediction and sensory evidence-a prediction error (PE)-should be signaled by increased gamma-band activity (GBA) in the cortical area where prediction and evidence are compared. This hypothesis contrasts with alternative accounts where violated predictions should lead to reduced neural responses. We tested these hypotheses by violating predictions about face orientation and illumination direction in a Mooney face-detection task, while recording magnetoencephalographic responses in a large sample of 48 human subjects. The investigated predictions, acquired via lifelong experience, are known to be processed at different time points and brain regions during face recognition.Behavioral responses confirmed the induction of PEs by our task. Beamformer source analysis revealed an early PE signal for unexpected orientation in visual brain areas followed by a PE signal for unexpected illumination in areas involved in 3D shape from shading and spatial working memory. Both PE signals were reflected by increases in high-frequency (68-140 Hz) GBA. In high-frequency GBA we also observed a late interaction effect in visual brain areas, probably corresponding to a high-level PE signal. In addition, increased high-frequency GBA for expected illumination was observed in brain areas involved in attention to internal representations. Our results strongly support the hypothesis that increased GBA signals PEs. Additionally, GBA may represent attentional effects.},
author = {Brodski, A. and Paasch, G.-F. and Helbling, S. and Wibral, M.},
doi = {10.1523/JNEUROSCI.1529-14.2015},
issn = {0270-6474},
journal = {Journal of Neuroscience},
keywords = {Mooney faces,gamma-band activity,perceptual closure,prediction error,predictive coding},
month = {jun},
number = {24},
pages = {8997--9006},
pmid = {26085625},
title = {{The Faces of Predictive Coding}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26085625 http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.1529-14.2015},
volume = {35},
year = {2015}
}
@article{Elias1955,
author = {Elias, P.},
doi = {10.1109/TIT.1955.1055116},
file = {:home/sflippl/Documents/Literature/Papers/Elias - 1955 - Predictive coding--II.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {mar},
number = {1},
pages = {24--33},
title = {{Predictive coding--II}},
url = {http://ieeexplore.ieee.org/document/1055116/},
volume = {1},
year = {1955}
}
@article{Elias1955a,
author = {Elias, P.},
doi = {10.1109/TIT.1955.1055126},
file = {:home/sflippl/Documents/Literature/Papers/Elias - 1955 - Predictive coding--I.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {mar},
number = {1},
pages = {16--24},
title = {{Predictive coding--I}},
url = {http://ieeexplore.ieee.org/document/1055126/},
volume = {1},
year = {1955}
}
@article{Friston2018,
abstract = {In the 20th century we thought the brain extracted knowledge from sensations. The 21st century witnessed a ‘strange inversion', in which the brain became an organ of inference, actively constructing explanations for what's going on ‘out there', beyond its sensory epithelia. One paper played a key role in this paradigm shift.},
author = {Friston, Karl},
doi = {10.1038/s41593-018-0200-7},
file = {:home/sflippl/Documents/Literature/Papers/Friston - 2018 - Does predictive coding have a future.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Computational neuroscience,Neural circuits,Neuronal physiology},
month = {aug},
number = {8},
pages = {1019--1021},
publisher = {Nature Publishing Group},
title = {{Does predictive coding have a future?}},
url = {http://www.nature.com/articles/s41593-018-0200-7},
volume = {21},
year = {2018}
}
@article{Barrett2018,
abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations that they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in-silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.},
archivePrefix = {arXiv},
arxivId = {1810.13373},
author = {Barrett, David G. T. and Morcos, Ari S. and Macke, Jakob H.},
eprint = {1810.13373},
month = {oct},
title = {{Analyzing biological and artificial neural networks: challenges with opportunities for synergy?}},
url = {http://arxiv.org/abs/1810.13373},
year = {2018}
}
@article{Petro2014,
abstract = {Closing the structure-function divide is more challenging in the brain than in any other organ (Lichtman and Denk, 2011). For example, in early visual cortex, feedback projections to V1 can be quantified (e.g. Budd, 1998) but the understanding of feedback function is comparatively rudimentary (Muckli and Petro, 2013). Focusing on the function of feedback, we discuss how textbook descriptions mask the complexity of V1 responses, and how feedback and local activity reflects not only sensory processing but internal brain states.},
author = {Petro, Lucy S. and Vizioli, Luca and Muckli, Lars},
doi = {10.3389/fpsyg.2014.01223},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Electrophsyiology,Feedback,V1,Vision,fMRI},
month = {nov},
pages = {1223},
publisher = {Frontiers},
title = {{Contributions of cortical feedback to sensory processing in primary visual cortex}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2014.01223/abstract},
volume = {5},
year = {2014}
}
@incollection{Adams2015,
abstract = {This chapter discusses how many features of cortical anatomy and physiology can be understood in the light of a predictive coding theory of brain function. In Sect. 7.1, we briefly discuss the theoretical reasons to suppose that the brain is likely to use predictive coding. One key theoretical underpinning of predictive coding is the free energy principle, which argues that brains must maximize the evidence for their (generative) model of sensory inputs: a process of 'active inference'. In Sect. 7.2, we discuss how active inference predicts com-monalities in the extrinsic connections of sensory and motor systems. Such com-monalities are found in their hierarchical structure (shown by laminar characteristics), their topography, their pharmacology and physiology. In Sect. 7.3, we show how the equations describing hierarchical message passing within a predictive coding scheme can be mapped on to key features of intrinsic connections, namely the canonical cortical microcircuit, and their implications for the oscillatory dynamics of different cell populations. In Sect. 7.4, we briefly review some empirical evidence for predictive coding in the brain. It might be thought impossible to specify the computations performed by the brain. However, there are some fairly fundamental constraints on the basic form of neuronal dynamics. The argument goes as follows – and can be regarded as a brief summary of the free energy principle (see Friston (2010) for details): • Biological systems are homoeostatic (or allostatic), which means that they minimise the dispersion (entropy) of their interoceptive and exteroceptive states. • Entropy is the average of surprise over time, which means biological systems minimise the surprise associated with their sensory states at each point in time. • In statistics, surprise is the negative logarithm of Bayesian model evidence, which means biological systems – like the brain – must continually maximise the Bayesian evidence for their (generative) model of sensory inputs. • Maximising Bayesian model evidence corresponds to Bayesian filtering of sensory inputs. This is also known as predictive coding. These arguments mean that by minimising surprise, through selecting appropri-ate sensations, the brain is implicitly maximising the evidence for its own existence – this is known as active inference. In other words, to maintain a homeostasis the brain must predict its sensory states on the basis of a model. Fulfilling those predictions corresponds to accumulating evidence for that model – and the brain that embodies it. The most popular scheme – for Bayesian filtering in neuronal circuits – is predic-tive coding (Rao and Ballard 1999). In this context, surprise corresponds (roughly) to prediction error. In predictive coding, top-down predictions are compared with bottom-up sensory information to form a prediction error. This prediction error is used to update higher-level representations – upon which top-down predictions are based. These optimised predictions then reduce prediction error at lower levels. To predict sensations, the brain must be equipped with a generative model of how its sensations are caused (Helmholtz 1866). A generative model describes how variables or causes in the environment conspire to produce sensory input. Gener-ative models map from (hidden) causes to (sensory) consequences. Perception then corresponds to the inverse mapping from sensations to their causes, while action can be thought of as the selective sampling of sensations. A special case of these models are hierarchical dynamic models (see Fig. 7.1), which grandfather most parametric models in statistics and machine learning (see Friston 2008). These models explain sensory data in terms of hidden causes and states, which mediate structural and dynamic dependencies respectively. Conditional dependencies among hidden states and causes are responsible for generating sensory input. These dependencies mean that we can interpret neuronal activity as message passing among the nodes of a generative model, wherein each node contains a canonical microcircuit that represents expectations about hidden states and causes (see Sect. 7.3). We now look at how perception or model inversion – recovering the hidden states and causes of this model given sensory data – might be implemented at the level of a microcircuit:},
author = {Adams, Rick A. and Friston, Karl J. and Bastos, Andre M.},
booktitle = {Recent Advances On The Modular Organization Of The Cortex},
doi = {10.1007/978-94-017-9900-3_7},
isbn = {9789401799003},
keywords = {Active inference,Free energy,Hierarchy,Microcircuit,Predictive coding},
title = {{Active inference, predictive coding and cortical architecture}},
year = {2015}
}
@article{Bosman2015,
abstract = {Regardless of major anatomical and neurodevelopmental differences, the vertebrate isocortex shows a remarkably well-conserved organization. In the isocortex, reciprocal connections between excitatory and inhibitory neurons are distributed across multiple layers, encompassing modular, dynamical and recurrent functional networks during information processing. These dynamical brain networks are often organized in neuronal assemblies interacting through rhythmic phase relationships. Accordingly, these oscillatory interactions are observed across multiple brain scale levels, and they are associated with several sensory, motor, and cognitive processes. Most notably, oscillatory interactions are also found in the complete spectrum of vertebrates. Yet, it is unknown why this functional organization is so well conserved in evolution. In this perspective, we propose some ideas about how functional requirements of the isocortex can account for the evolutionary stability observed in microcircuits across vertebrates. We argue that isocortex architectures represent canonical microcircuits resulting from: (i) the early selection of neuronal architectures based on the oscillatory excitatory-inhibitory balance, which lead to the implementation of compartmentalized oscillations and (ii) the subsequent emergence of inferential coding strategies (predictive coding), which are able to expand computational capacities. We also argue that these functional constraints may be the result of several advantages that oscillatory activity contributes to brain network processes, such as information transmission and code reliability. In this manner, similarities in mesoscale brain circuitry and input-output organization between different vertebrate groups may reflect evolutionary constraints imposed by these functional requirements, which may or may not be traceable to a common ancestor.},
author = {Bosman, Conrado A. and Aboitiz, Francisco},
doi = {10.3389/fnins.2015.00303},
isbn = {1662-4548 (Print)$\backslash$r1662-453X (Linking)},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Canonical microcircuits,Cortical evolution,Cortical neurodevelopment,Neuronal oscillations,Predictive coding},
pmid = {26388716},
title = {{Functional constraints in the evolution of brain circuits}},
year = {2015}
}
@article{Clark2013,
abstract = {{\textless}p{\textgreater}Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this “hierarchical prediction machine” approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.{\textless}/p{\textgreater}},
author = {Clark, Andy},
doi = {10.1017/S0140525X12000477},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
month = {jun},
number = {03},
pages = {181--204},
pmid = {23663408},
title = {{Whatever next? Predictive brains, situated agents, and the future of cognitive science}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23663408 http://www.journals.cambridge.org/abstract{\_}S0140525X12000477},
volume = {36},
year = {2013}
}
@article{Giraud2018,
abstract = {Predictive coding and neural oscillations are two descriptive levels of brain functioning whose overlap is not yet understood. Chao et al. (2018) now show that hierarchical predictive coding is instantiated by asymmetric information channeling in the $\gamma$ and $\alpha$/$\beta$ oscillatory ranges.},
author = {Giraud, Anne-Lise and Arnal, Luc H.},
doi = {10.1016/J.NEURON.2018.11.020},
issn = {0896-6273},
journal = {Neuron},
month = {dec},
number = {5},
pages = {1022--1024},
publisher = {Cell Press},
title = {{Hierarchical Predictive Information Is Channeled by Asymmetric Oscillatory Activity}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627318310110?via{\%}3Dihub},
volume = {100},
year = {2018}
}
@article{Chao2018,
abstract = {According to predictive-coding theory, cortical areas continuously generate and update predictions of sensory inputs at different hierarchical levels and emit prediction errors when the predicted and actual inputs differ. However, predictions and prediction errors are simultaneous and interdependent processes, making it difficult to disentangle their constituent neural network organization. Here, we test the theory by using high-density electrocorticography (ECoG) in monkeys during an auditory "local-global" paradigm in which the temporal regularities of the stimuli were controlled at two hierarchical levels. We decomposed the broadband data and identified lower- and higher-level prediction-error signals in early auditory cortex and anterior temporal cortex, respectively, and a prediction-update signal sent from prefrontal cortex back to temporal cortex. The prediction-error and prediction-update signals were transmitted via $\gamma$ ({\textgreater}40 Hz) and $\alpha$/$\beta$ ({\textless}30 Hz) oscillations, respectively. Our findings provide strong support for hierarchical predictive coding and outline how it is dynamically implemented using distinct cortical areas and frequencies.},
author = {Chao, Zenas C and Takaura, Kana and Wang, Liping and Fujii, Naotaka and Dehaene, Stanislas},
doi = {10.1016/j.neuron.2018.10.004},
issn = {1097-4199},
journal = {Neuron},
month = {dec},
number = {5},
pages = {1252--1266.e3},
pmid = {30482692},
publisher = {Elsevier},
title = {{Large-Scale Cortical Networks for Hierarchical Prediction and Prediction Error in the Primate Brain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30482692},
volume = {100},
year = {2018}
}
@article{Hopfield1984,
abstract = {A model for a large network of "neurons" with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological "neurons." Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
author = {Hopfield, J J},
doi = {10.1073/PNAS.81.10.3088},
file = {:home/sflippl/Documents/Literature/Papers/Hopfield - 1984 - Neurons with graded response have collective computational properties like those of two-state neurons.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {may},
number = {10},
pages = {3088--92},
pmid = {6587342},
publisher = {National Academy of Sciences},
title = {{Neurons with graded response have collective computational properties like those of two-state neurons.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6587342 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC345226},
volume = {81},
year = {1984}
}
@article{Xie2003,
abstract = {Backpropagation and contrastive Hebbian learning are two methods of training networks with hidden neurons. Backpropagation computes an error signal for the output neurons and spreads it over the hidden neurons. Contrastive Hebbian learning involves clamping the output neurons at desired values and letting the effect spread through feedback connections over the entire network. To investigate the relationship between these two forms of learning, we consider a special case in which they are identical: a multilayer perceptron with linear output units, to which weak feedback connections have been added. In this case, the change in network state caused by clamping the output neurons turns out to be the same as the error signal spread by backpropagation, except for a scalar prefactor. This suggests that the functionality of backpropagation can be realized alternatively by a Hebbian-type learning algorithm, which is suitable for implementation in biological networks.},
author = {Xie, Xiaohui and Seung, H. Sebastian},
doi = {10.1162/089976603762552988},
file = {::},
issn = {0899-7667},
journal = {Neural Computation},
month = {feb},
number = {2},
pages = {441--454},
publisher = { MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu  },
title = {{Equivalence of Backpropagation and Contrastive Hebbian Learning in a Layered Network}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976603762552988},
volume = {15},
year = {2003}
}
@article{Detorakis2018,
abstract = {Neural networks are commonly trained to make predictions through learning algorithms. Contrastive Hebbian learning, which is a powerful rule inspired by gradient backpropagation, is based on Hebb's rule and the contrastive divergence algorithm. It operates in two phases, the forward (or free) phase, where the data are fed to the network, and a backward (or clamped) phase, where the target signals are clamped to the output layer of the network and the feedback signals are transformed through the transpose synaptic weight matrices. This implies symmetries at the synaptic level, for which there is no evidence in the brain. In this work, we propose a new variant of the algorithm, called random contrastive Hebbian learning, which does not rely on any synaptic weights symmetries. Instead, it uses random matrices to transform the feedback signals during the clamped phase, and the neural dynamics are described by first order non-linear differential equations. The algorithm is experimentally verified by solving a Boolean logic task, classification tasks (handwritten digits and letters), and an autoencoding task. This article also shows how the parameters affect learning, especially the random matrices. We use the pseudospectra analysis to investigate further how random matrices impact the learning process. Finally, we discuss the biological plausibility of the proposed algorithm, and how it can give rise to better computational models for learning.},
archivePrefix = {arXiv},
arxivId = {1806.07406},
author = {Detorakis, Georgios and Bartley, Travis and Neftci, Emre},
eprint = {1806.07406},
file = {::},
month = {jun},
title = {{Contrastive Hebbian Learning with Random Feedback Weights}},
url = {http://arxiv.org/abs/1806.07406},
year = {2018}
}
@article{Movellan1991,
abstract = {This paper shows that contrastive Hebbian, the algorithm used in mean field learning, can be applied to any continuous Hopfield model. This implies that non-logistic activation functions as well as self connections are allowed. Contrary to previous approaches, the learning algorithm is derived without considering it a mean field approximation to Boltzmann machine learning. The paper includes a discussion of the conditions under which the function that contrastive Hebbian minimizes can be considered a proper error function, and an analysis of five different training regimes. An appendix provides complete demonstrations and specific instructions on how to implement contrastive Hebbian learning in interactive activation and competition models (a convenient version of the continuous Hopfield model).},
author = {Movellan, Javier R.},
doi = {10.1016/B978-1-4832-1448-1.50007-X},
file = {:home/sflippl/Documents/Literature/Papers/Movellan - 1991 - Contrastive Hebbian Learning in the Continuous Hopfield Model.pdf:pdf},
isbn = {9781483214481},
journal = {Connectionist Models},
month = {jan},
pages = {10--17},
publisher = {Morgan Kaufmann},
title = {{Contrastive Hebbian Learning in the Continuous Hopfield Model}},
url = {https://www.sciencedirect.com/science/article/pii/B978148321448150007X},
year = {1991}
}
@article{Wilmes2018,
author = {Wilmes, Katharina Anna and Clopath, Claudia},
journal = {bioRxiv},
title = {{Inhibitory microcircuits for top-down plasticity of sensory representations}},
year = {2018}
}
@article{Funahashi1989,
abstract = {In this paper, we prove that any continuous mapping can be approximately realized by Rumelhart-Hinton-Williams' multilayer neural networks with at least one hidden layer whose output functions are sigmoid functions. The starting point of the proof for the one hidden layer case is an integral formula recently proposed by Irie-Miyake and from this, the general case (for any number of hidden layers) can be proved by induction. The two hidden layers case is proved also by using the Kolmogorov-Arnold-Sprecher theorem and this proof also gives non-trivial realizations.},
author = {Funahashi, Ken-Ichi},
doi = {10.1016/0893-6080(89)90003-8},
file = {:home/sflippl/Documents/Literature/Papers/Funahashi - 1989 - On the approximate realization of continuous mappings by neural networks.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
month = {jan},
number = {3},
pages = {183--192},
publisher = {Pergamon},
title = {{On the approximate realization of continuous mappings by neural networks}},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900038},
volume = {2},
year = {1989}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
file = {:home/sflippl/Documents/Literature/Papers/Hornik, Stinchcombe, White - 1989 - Multilayer feedforward networks are universal approximators.pdf:pdf},
issn = {0893-6080},
journal = {Neural Networks},
month = {jan},
number = {5},
pages = {359--366},
publisher = {Pergamon},
title = {{Multilayer feedforward networks are universal approximators}},
url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
volume = {2},
year = {1989}
}
@article{Bastos2012,
abstract = {This Perspective considers the influential notion of a canonical (cortical) microcircuit in light of recent theories about neuronal processing. Specifically, we conciliate quantitative studies of microcircuitry and the functional logic of neuronal computations. We revisit the established idea that message passing among hierarchical cortical areas implements a form of Bayesian inference-paying careful attention to the implications for intrinsic connections among neuronal populations. By deriving canonical forms for these computations, one can associate specific neuronal populations with specific computational roles. This analysis discloses a remarkable correspondence between the microcircuitry of the cortical column and the connectivity implied by predictive coding. Furthermore, it provides some intuitive insights into the functional asymmetries between feedforward and feedback connections and the characteristic frequencies over which they operate.},
author = {Bastos, Andre M and Usrey, W Martin and Adams, Rick A and Mangun, George R and Fries, Pascal and Friston, Karl J},
doi = {10.1016/j.neuron.2012.10.038},
file = {:home/sflippl/Documents/Literature/Papers/Bastos et al. - 2012 - Canonical microcircuits for predictive coding.pdf:pdf},
issn = {1097-4199},
journal = {Neuron},
month = {nov},
number = {4},
pages = {695--711},
pmid = {23177956},
publisher = {NIH Public Access},
title = {{Canonical microcircuits for predictive coding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23177956 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3777738},
volume = {76},
year = {2012}
}
@article{Dayan1995,
abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
doi = {10.1162/neco.1995.7.5.889},
file = {:home/sflippl/Documents/Literature/Papers/Dayan et al. - 1995 - The Helmholtz Machine.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {sep},
number = {5},
pages = {889--904},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142‐1046 USA journals-info@mit.edu},
title = {{The Helmholtz Machine}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1995.7.5.889},
volume = {7},
year = {1995}
}
@misc{nyu-debate,
abstract = {https://www.youtube.com/watch?v=CvAbPtbjxhw},
title = {{NYU Debate: Does Hierarchical Predictive Coding Explain Perception?}},
url = {https://wp.nyu.edu/consciousness/predictive-coding/},
urldate = {2018-12-10}
}
@article{Koch1999,
abstract = {Predicting the visual world: silence is golden},
author = {Koch, Christof and Poggio, Tomaso},
doi = {10.1038/4511},
file = {::},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jan},
number = {1},
pages = {9--10},
publisher = {Nature Publishing Group},
title = {{Predicting the visual world: silence is golden}},
url = {http://www.nature.com/articles/nn0199{\_}9},
volume = {2},
year = {1999}
}
@article{Maunsell1983,
abstract = {The cortical and subcortical connections of the middle temporal visual area (MT) of the macaque monkey were investigated using combined injections of [3H]proline and horseradish peroxidase within MT. Cortical connections were assigned to specific visual areas on the basis of their relationship to the pattern of interhemispheric connections, revealed by staining for degeneration following callosal transection. MT was shown to be reciprocally connected with many topographically organized cortical visual areas, including V1, V2, V3, and V4. These pathways link regions representing corresponding portions of the visual field in the different areas. In addition, MT has reciprocal connections with two previously unidentified cortical areas, which we have designated the medial superior temporal area (MST) and the ventral intraparietal area (VIP). The laminar distribution of terminals and cell bodies in cortical areas connected with MT follows a consistent pattern. In areas V1, V2, and V3, the projections to MT arise largely or exclusively from cells in supragranular layers, and the reciprocal connections from MT terminate mainly in supragranular and infragranular layers. In contrast, the projections to MST and VIP terminate mainly in layer IV, and the reciprocal pathways originate from cells in both superficial and deep layers. On the basis of this pattern, each connection can be designated as forward or feedback in nature, and a hierarchical arrangement of visual areas can be determined. In this hierarchy, MT is at a higher level than V1, V2, and V3, and at a lower level than MST and VIP. Subcortical projections were seen from MT to the claustrum, the putamen, the caudate nucleus, the inferior and lateral subdivisions of the pulvinar complex, the ventral lateral geniculate nucleus, the reticular nucleus of the thalamus, the superior colliculus, and the pontine nuclei.},
author = {Maunsell, J H and van Essen, D C},
doi = {10.1523/JNEUROSCI.03-12-02563.1983},
file = {::},
issn = {0270-6474},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
month = {dec},
number = {12},
pages = {2563--86},
pmid = {6655500},
publisher = {Society for Neuroscience},
title = {{The connections of the middle temporal visual area (MT) and their relationship to a cortical hierarchy in the macaque monkey.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/6655500},
volume = {3},
year = {1983}
}
@article{Kok2012,
author = {Kok, Peter and Rahnev, Dobromir and Jehee, Janneke F. M. and Lau, Hakwan C. and de Lange, Floris P.},
doi = {10.1093/cercor/bhr310},
file = {:home/sflippl/Documents/Literature/Papers/Kok et al. - 2012 - Attention Reverses the Effect of Prediction in Silencing Sensory Signals.pdf:pdf},
issn = {1460-2199},
journal = {Cerebral Cortex},
keywords = {attenuation,precision,primary visual cortex,visual cortex},
month = {sep},
number = {9},
pages = {2197--2206},
publisher = {Oxford University Press},
title = {{Attention Reverses the Effect of Prediction in Silencing Sensory Signals}},
url = {https://academic.oup.com/cercor/article-lookup/doi/10.1093/cercor/bhr310},
volume = {22},
year = {2012}
}
@article{Kogo2015,
annote = {This article points out a conflict between the pervasive view that error neurons "attempt" to shut up, and many models with which this is incompatible.},
author = {Kogo, Naoki and Trengove, Chris},
doi = {10.3389/fncom.2015.00111},
file = {:home/sflippl/Documents/Literature/Papers/Kogo, Trengove - 2015 - Is predictive coding theory articulated enough to be testable.pdf:pdf},
issn = {1662-5188},
journal = {Frontiers in computational neuroscience},
keywords = {bayesian models,error signals,feedback,generative model,neuroanatomy,physiological,predictive coding,visual cortex},
number = {111},
pages = {1--4},
pmid = {26441621},
publisher = {Frontiers Media SA},
title = {{Is predictive coding theory articulated enough to be testable?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26441621 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4561670},
volume = {9},
year = {2015}
}
@article{Walker2002,
abstract = {By definition, the region outside the classical receptive field (CRF) of a neuron in the visual cortex does not directly activate the cell. However, the response of a neuron can be influenced by stimulation of the surrounding area. In previous work, we showed that this influence is mainly suppressive and that it is generally limited to a local region outside the CRF. In the experiments reported here, we investigate the mechanisms of the suppressive effect. Our approach is to find the position of a grating patch that is most effective in suppressing the response of a cell. We then use a masking stimulus at different contrasts over the grating patch in an attempt to disinhibit the response. We find that suppressive effects may be partially or completely reversed by use of the masking stimulus. This disinhibition suggests that effects from outside the CRF may be local. Although they do not necessarily underlie the perceptual analysis of a figure-ground visual scene, they may provide a substrate for this process.},
author = {Walker, Gary A and Ohzawa, Izumi and Freeman, Ralph D and Miller, KD. and Bullier, J. and Lund, JS.},
doi = {20026492},
file = {::},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
month = {jul},
number = {13},
pages = {5659--68},
pmid = {12097517},
publisher = {Society for Neuroscience},
title = {{Disinhibition outside receptive fields in the visual cortex.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12097517},
volume = {22},
year = {2002}
}
@article{Spratling2009,
abstract = {This paper demonstrates that nonnegative matrix factorisation is mathematically related to a class of neural networks that employ negative feedback as a mechanism of competition. This observation inspires a novel learning algorithm which we call Divisive Input Modulation (DIM). The proposed algorithm provides a mathematically simple and computationally efficient method for the unsupervised learning of image components, even in conditions where these elementary features overlap considerably. To test the proposed algorithm, a novel artificial task is introduced which is similar to the frequently-used bars problem but employs squares rather than bars to increase the degree of overlap between components. Using this task, we investigate how the proposed method performs on the parsing of artificial images composed of overlapping features, given the correct representation of the individual components; and secondly, we investigate how well it can learn the elementary components from artificial training images. We compare the performance of the proposed algorithm with its predecessors including variations on these algorithms that have produced state-of-the-art performance on the bars problem. The proposed algorithm is more successful than its predecessors in dealing with overlap and occlusion in the artificial task that has been used to assess performance.},
author = {Spratling, M W and {De Meyer}, K and Kompass, R},
doi = {10.1155/2009/381457},
file = {:home/sflippl/Documents/Literature/Papers/Spratling, De Meyer, Kompass - 2009 - Unsupervised learning of overlapping image components using divisive input modulation.pdf:pdf},
issn = {1687-5273},
journal = {Computational intelligence and neuroscience},
month = {may},
pages = {381457},
pmid = {19424442},
publisher = {Hindawi},
title = {{Unsupervised learning of overlapping image components using divisive input modulation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19424442 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2677748},
volume = {2009},
year = {2009}
}
@article{Harpur1996,
author = {Harpur, George F and Prager, Richard W},
doi = {10.1088/0954-898X_7_2_007},
file = {:home/sflippl/Documents/Literature/Papers/Harpur, Prager - 1996 - Development of low entropy coding in a recurrent network.pdf:pdf},
issn = {0954-898X},
journal = {Network: Computation in Neural Systems},
month = {jan},
number = {2},
pages = {277--284},
title = {{Development of low entropy coding in a recurrent network}},
url = {https://www.tandfonline.com/doi/full/10.1088/0954-898X{\_}7{\_}2{\_}007},
volume = {7},
year = {1996}
}
@article{Attneave1954,
author = {Attneave, Fred},
doi = {10.1037/h0054663},
file = {:home/sflippl/Documents/Literature/Papers/Attneave - 1954 - Some informational aspects of visual perception.pdf:pdf},
issn = {1939-1471},
journal = {Psychological Review},
number = {3},
pages = {183--193},
title = {{Some informational aspects of visual perception.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0054663},
volume = {61},
year = {1954}
}
@article{Olshausen1996,
author = {Olshausen, Bruno A. and Field, David J.},
file = {:home/sflippl/Documents/Literature/Papers/Olshausen, Field - 1996 - Natural image statistics and efficient coding.pdf:pdf},
journal = {undefined},
title = {{Natural image statistics and efficient coding.}},
url = {https://www.semanticscholar.org/paper/Natural-image-statistics-and-efficient-coding.-Olshausen-Field/009e9f35f2f6e95f5220e46dad98ac58d220e0ac},
year = {1996}
}
@article{Oliver1952,
author = {Oliver, B. M.},
doi = {10.1002/j.1538-7305.1952.tb01403.x},
issn = {00058580},
journal = {Bell System Technical Journal},
month = {jul},
number = {4},
pages = {724--750},
title = {{Efficient Coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6771050},
volume = {31},
year = {1952}
}
@article{Harrison1952,
author = {Harrison, C. W.},
doi = {10.1002/j.1538-7305.1952.tb01405.x},
issn = {00058580},
journal = {Bell System Technical Journal},
month = {jul},
number = {4},
pages = {764--783},
title = {{Experiments with Linear Prediction in Television}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6771052},
volume = {31},
year = {1952}
}
@article{Koelsch2018,
abstract = {We suggest that music perception is an active act of listening, providing an irresistible epistemic offering. When listening to music we constantly generate plausible hypotheses about what could happen next, while actively attending to music resolves the ensuing uncertainty. Within the predictive coding framework, we present a novel formulation of precision filtering and attentional selection, which explains why some lower-level auditory, and even higher-level music-syntactic processes elicited by irregular events are relatively exempt from top-down predictive processes. We review findings providing unique evidence for the attentional selection of salient auditory features. This formulation suggests that 'listening' is a more active process than traditionally conceived in models of perception.},
author = {Koelsch, Stefan and Vuust, Peter and Friston, Karl},
doi = {10.1016/j.tics.2018.10.006},
issn = {1879-307X},
journal = {Trends in cognitive sciences},
keywords = {ERAN,MMN,active inference,auditory processing,embodiment,music perception,predictive coding},
month = {nov},
number = {0},
pmid = {30471869},
publisher = {Elsevier},
title = {{Predictive Processes and the Peculiar Case of Music.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30471869},
volume = {0},
year = {2018}
}
@article{Sohn2018,
abstract = {Statistical regularities in the environment create prior beliefs that we rely on to optimize our behavior when sensory information is uncertain. Bayesian theory formalizes how prior beliefs can be leveraged, and has had a major impact on models of perception, sensorimotor function, and cognition. However, it is not known how recurrent interactions among neurons mediate Bayesian integration. Using a time interval reproduction task in monkeys, we found that prior statistics warp the underlying structure of population activity in the frontal cortex allowing the mapping of sensory inputs to motor outputs to be biased in accordance with Bayesian inference. Analysis of neural network models performing the task revealed that this warping was mediated by a low-dimensional curved manifold, and allowed us to further probe the potential causal underpinnings of this computational strategy. These results uncover a simple and general principle whereby prior beliefs exert their influence on behavior by sculpting cortical latent dynamics.},
author = {Sohn, Hansem and Narain, Devika and Meirhaeghe, Nicolas and Jazayeri, Mehrdad},
doi = {10.1101/465419},
journal = {bioRxiv},
month = {nov},
pages = {465419},
publisher = {Cold Spring Harbor Laboratory},
title = {{Bayesian computation through cortical latent dynamics}},
url = {https://www.biorxiv.org/content/early/2018/11/08/465419},
year = {2018}
}
@article{Lotter2016,
abstract = {While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network ("PredNet") architecture that is inspired by the concept of "predictive coding" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. Altogether, these results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.},
archivePrefix = {arXiv},
arxivId = {1605.08104},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1605.08104},
month = {may},
title = {{Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning}},
url = {http://arxiv.org/abs/1605.08104},
year = {2016}
}
@article{Lotter2018,
abstract = {While deep neural networks take loose inspiration from neuroscience, it is an open question how seriously to take the analogies between artificial deep networks and biological neuronal systems. Interestingly, recent work has shown that deep convolutional neural networks (CNNs) trained on large-scale image recognition tasks can serve as strikingly good models for predicting the responses of neurons in visual cortex to visual stimuli, suggesting that analogies between artificial and biological neural networks may be more than superficial. However, while CNNs capture key properties of the average responses of cortical neurons, they fail to explain other properties of these neurons. For one, CNNs typically require large quantities of labeled input data for training. Our own brains, in contrast, rarely have access to this kind of supervision, so to the extent that representations are similar between CNNs and brains, this similarity must arise via different training paths. In addition, neurons in visual cortex produce complex time-varying responses even to static inputs, and they dynamically tune themselves to temporal regularities in the visual environment. We argue that these differences are clues to fundamental differences between the computations performed in the brain and in deep networks. To begin to close the gap, here we study the emergent properties of a previously-described recurrent generative network that is trained to predict future video frames in a self-supervised manner. Remarkably, the model is able to capture a wide variety of seemingly disparate phenomena observed in visual cortex, ranging from single unit response dynamics to complex perceptual motion illusions. These results suggest potentially deep connections between recurrent predictive neural network models and the brain, providing new leads that can enrich both fields.},
archivePrefix = {arXiv},
arxivId = {1805.10734},
author = {Lotter, William and Kreiman, Gabriel and Cox, David},
eprint = {1805.10734},
month = {may},
title = {{A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception}},
url = {http://arxiv.org/abs/1805.10734},
year = {2018}
}
@article{Dora2018,
abstract = {It has been argued that the brain is a prediction machine that continuously learns how to make better predictions about the stimuli received from the external environment. It builds a model of the world around us and uses this model to infer the external stimulus. Predictive coding has been proposed as a mechanism through which the brain might be able to build such a model of the external environment. However, it is not clear how predictive coding can be used to build deep neural network models of the brain while complying with the architectural constraints imposed by the brain. In this paper, we describe an algorithm to build a deep generative model using predictive coding that can be used to infer latent representations about the stimuli received from external environment. Specifically, we used predictive coding to train a deep neural network on real-world images in a unsupervised learning paradigm. To understand the capacity of the network with regards to modeling the external environment, we studied the latent representations generated by the model on images of objects that are never presented to the model during training. Despite the novel features of these objects the model is able to infer the latent representations for them. Furthermore, the reconstructions of the original images obtained from these latent representations preserve the important details of these objects.},
author = {Dora, Shirin and Pennartz, Cyriel M A and Bohte, Sander M},
doi = {10.1101/278218},
journal = {bioRxiv},
month = {mar},
pages = {278218},
publisher = {Cold Spring Harbor Laboratory},
title = {{A Deep Predictive Coding Network for Learning Latent Representations}},
url = {https://www.biorxiv.org/content/early/2018/03/26/278218},
year = {2018}
}
@article{Hovsepyan2018,
abstract = {Speech comprehension requires segmenting continuous speech to connect it on-line with discrete linguistic neural representations. This process relies on theta-gamma oscillation coupling, which tracks syllables and encodes them in decipherable neural activity. Speech comprehension also strongly depends on contextual cues predicting speech structure and content. To explore the effects of theta-gamma coupling on bottom-up/top-down dynamics during on-line speech perception, we designed a generative model that can recognize syllable sequences in continuous speech. The model uses theta oscillations to detect syllable onsets and align both gamma-rate encoding activity with syllable boundaries and predictions with speech input. We observed that the model performed best when theta oscillations were used to align gamma units with input syllables, i.e. when bidirectional information flows were coordinated, and internal timing knowledge was exploited. This work demonstrates that notions of predictive coding and neural oscillations can usefully be brought together to account for dynamic on-line sensory processing.},
author = {Hovsepyan, Sevada and Olasagasti, Itsaso and Giraud, Anne-Lise},
doi = {10.1101/477588},
journal = {bioRxiv},
month = {nov},
pages = {477588},
publisher = {Cold Spring Harbor Laboratory},
title = {{Combining predictive coding with neural oscillations optimizes on-line speech processing}},
url = {https://www.biorxiv.org/content/early/2018/11/27/477588},
year = {2018}
}
@article{Hogendoorn2018,
abstract = {Hierarchical predictive coding is an influential model of cortical organization, in which sequential hierarchical layers are connected by feedback connections carrying predictions, as well as feedforward connections carrying prediction errors. To date, however, predictive coding models have neglected to take into account that neural transmission itself takes time. For a time-varying stimulus, such as a moving object, this means that feedback predictions become misaligned with new sensory input. We present an extended model implementing both feed-forward and feedback extrapolation mechanisms that realigns feedback predictions to minimize prediction error. This realignment has the consequence that neural representations across all hierarchical stages become aligned in real-time. Using visual motion as an example, we show that the model is neurally plausible, that it is consistent with evidence of extrapolation mechanisms throughout the visual hierarchy, that it predicts several known motion-position illusions, and that it provides a solution to the temporal binding problem.},
author = {Hogendoorn, Hinze and Burkitt, Anthony N},
doi = {10.1101/453183},
journal = {bioRxiv},
month = {oct},
pages = {453183},
publisher = {Cold Spring Harbor Laboratory},
title = {{Predictive coding with neural transmission delays: a real-time temporal alignment hypothesis}},
url = {https://www.biorxiv.org/content/early/2018/10/25/453183},
year = {2018}
}
@article{Vasser2018,
abstract = {It is well known that the human brain continuously predicts the sensory consequences of its own body movements, which typically results in sensory attenuation. Yet, the extent and exact mechanisms underlying sensory attenuation are still debated. To explore this issue, we asked participants to decide which of two visual stimuli was of higher contrast in a virtual reality situation where one of the stimuli could appear behind the participants' invisible moving hand or not. Over two experiments, we measured the effects of such "virtual occlusion" on first-order sensitivity and on metacognitive monitoring. Our findings show that self-generated hand movements reduced the apparent contrast of the stimulus. This result can be explained by the active inference theory. Moreover, sensory attenuation seemed to affect only first-order sensitivity and not (second-order) metacognitive judgments of confidence.},
author = {Vasser, Madis and Vuillaume, Laur{\`{e}}ne and Cleeremans, Axel and Aru, Jaan},
doi = {10.1101/474783},
journal = {bioRxiv},
month = {nov},
pages = {474783},
publisher = {Cold Spring Harbor Laboratory},
title = {{Waving goodbye to contrast: Self-generated hand movements attenuate visual sensitivity}},
url = {https://www.biorxiv.org/content/early/2018/11/21/474783},
year = {2018}
}
@article{Palmer2015,
abstract = {Guiding behavior requires the brain to make predictions about the future values of sensory inputs. Here, we show that efficient predictive computation starts at the earliest stages of the visual system. We compute how much information groups of retinal ganglion cells carry about the future state of their visual inputs and show that nearly every cell in the retina participates in a group of cells for which this predictive information is close to the physical limit set by the statistical structure of the inputs themselves. Groups of cells in the retina carry information about the future state of their own activity, and we show that this information can be compressed further and encoded by downstream predictor neurons that exhibit feature selectivity that would support predictive computations. Efficient representation of predictive information is a candidate principle that can be applied at each stage of neural computation.},
author = {Palmer, Stephanie E and Marre, Olivier and Berry, Michael J and Bialek, William},
doi = {10.1073/pnas.1506855112},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {information theory,neural coding,retina},
month = {jun},
number = {22},
pages = {6908--13},
pmid = {26038544},
publisher = {National Academy of Sciences},
title = {{Predictive information in a sensory population.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26038544 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4460449},
volume = {112},
year = {2015}
}
@article{Hogendoorn2018a,
abstract = {Hierarchical predictive coding is an influential model of cortical organization, in which sequential hierarchical layers are connected by feedback connections carrying predictions, as well as feedforward connections carrying prediction errors. To date, however, predictive coding models have neglected to take into account that neural transmission itself takes time. For a time-varying stimulus, such as a moving object, this means that feedback predictions become misaligned with new sensory input. We present an extended model implementing both feed-forward and feedback extrapolation mechanisms that realigns feedback predictions to minimize prediction error. This realignment has the consequence that neural representations across all hierarchical stages become aligned in real-time. Using visual motion as an example, we show that the model is neurally plausible, that it is consistent with evidence of extrapolation mechanisms throughout the visual hierarchy, that it predicts several known motion-position illusions, and that it provides a solution to the temporal binding problem.},
author = {Hogendoorn, Hinze and Burkitt, Anthony N},
doi = {10.1101/453183},
journal = {bioRxiv},
month = {oct},
pages = {453183},
publisher = {Cold Spring Harbor Laboratory},
title = {{Predictive coding with neural transmission delays: a real-time temporal alignment hypothesis}},
url = {https://www.biorxiv.org/content/early/2018/10/25/453183},
year = {2018}
}
@article{Spratling2013,
abstract = {{\textless}p{\textgreater}It is often helpful to distinguish between a theory (Marr's computational level) and a specific implementation of that theory (Marr's physical level). However, in the target article, a single implementation of predictive coding is presented as if this were the theory of predictive coding itself. Other implementations of predictive coding have been formulated which can explain additional neurobiological phenomena.{\textless}/p{\textgreater}},
author = {Spratling, Michael W.},
doi = {10.1017/S0140525X12002178},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
month = {jun},
number = {03},
pages = {231--232},
publisher = {Cambridge University Press},
title = {{Distinguishing theory from implementation in predictive coding accounts of brain function}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X12002178},
volume = {36},
year = {2013}
}
@article{Spratling2017,
abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term "predictive coding". This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
author = {Spratling, M.W.},
doi = {10.1016/j.bandc.2015.11.003},
file = {:home/sflippl/Documents/Literature/Papers/Spratling - 2017 - A review of predictive coding algorithms.pdf:pdf},
issn = {02782626},
journal = {Brain and Cognition},
keywords = {Cortex,Free energy,Neural networks,Predictive coding,Retina,Signal processing},
month = {mar},
pages = {92--97},
pmid = {26809759},
title = {{A review of predictive coding algorithms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26809759 https://linkinghub.elsevier.com/retrieve/pii/S027826261530035X},
volume = {112},
year = {2017}
}
@article{Spratling2008,
abstract = {A simple variation of the standard biased competition model is shown, via some trivial mathematical manipulations, to be identical to predictive coding. Specifically, it is shown that a particular implementation of the biased competition model, in which nodes compete via inhibition that targets the inputs to a cortical region, is mathematically equivalent to the linear predictive coding model. This observation demonstrates that these two important and influential rival theories of cortical function are minor variations on the same underlying mathematical model.},
author = {Spratling, Michael W.},
doi = {10.3389/neuro.10.004.2008},
issn = {16625188},
journal = {Frontiers in Computational Neuroscience},
keywords = {biased competition,cortical circuits,cortical feedback,neural networks,predictive coding},
pages = {4},
pmid = {18978957},
title = {{Reconciling Predictive Coding and Biased Competition Models of Cortical Function}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18978957 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC2576514 http://journal.frontiersin.org/article/10.3389/neuro.10.004.2008/abstract},
volume = {2},
year = {2008}
}
@article{DenOuden2012,
abstract = {Prediction errors (PE) are a central notion in theoretical models of reinforcement learning, perceptual inference, decision-making and cognition, and prediction error signals have been reported across a wide range of brain regions and experimental paradigms. Here, we will make an attempt to see the forest for the trees and consider the commonalities and differences of reported PE signals in light of recent suggestions that the computation of PE forms a fundamental mode of brain function. We discuss where different types of PE are encoded, how they are generated, and the different functional roles they fulfill. We suggest that while encoding of PE is a common computation across brain regions, the content and function of these error signals can be very different and are determined by the afferent and efferent connections within the neural circuitry in which they arise.},
author = {den Ouden, Hanneke E. M. and Kok, Peter and de Lange, Floris P.},
doi = {10.3389/fpsyg.2012.00548},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {decision-making,expectation,learning,perceptual inference,prediction,prediction error,predictive coding},
pages = {548},
pmid = {23248610},
title = {{How Prediction Errors Shape Perception, Attention, and Motivation}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23248610 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3518876 http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00548/abstract},
volume = {3},
year = {2012}
}
@article{Gotts2012,
abstract = {Stimulus repetition in identification tasks leads to improved behavioral performance ("repetition priming") but attenuated neural responses ("repetition suppression") throughout task-engaged cortical regions. While it's clear that this pervasive brain-behavior relationship reflects some form of improved processing efficiency, the exact form that it takes remains elusive. In this Discussion Paper, we review four different theoretical proposals that have the potential to link repetition suppression and priming, with a particular focus on a proposal that stimulus repetition affects improved efficiency through enhanced neural synchronization. We argue that despite exciting recent work on the role of neural synchronization in cognitive processes such as attention and perception, similar studies in the domain of learning and memory - and priming, in particular - have been lacking. We emphasize the need for new studies with adequate spatiotemporal resolution, formulate several novel predictions, and discuss our ongoing efforts to disentangle the current proposals.},
author = {Gotts, Stephen J. and Chow, Carson C. and Martin, Alex},
doi = {10.1080/17588928.2012.670617},
issn = {1758-8928},
journal = {Cognitive Neuroscience},
month = {sep},
number = {3-4},
pages = {227--237},
pmid = {23144664},
title = {{Repetition priming and repetition suppression: A case for enhanced efficiency through neural synchronization}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23144664 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3491809 http://www.tandfonline.com/doi/abs/10.1080/17588928.2012.670617},
volume = {3},
year = {2012}
}
@article{Summerfield2014,
abstract = {Sensory signals are highly structured in both space and time. These structural regularities in visual information allow expectations to form about future stimulation, thereby facilitating decisions about visual features and objects. Here, we discuss how expectation modulates neural signals and behaviour in humans and other primates. We consider how expectations bias visual activity before a stimulus occurs, and how neural signals elicited by expected and unexpected stimuli differ. We discuss how expectations may influence decision signals at the computational level. Finally, we consider the relationship between visual expectation and related concepts, such as attention and adaptation.},
author = {Summerfield, Christopher and de Lange, Floris P.},
doi = {10.1038/nrn3838},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {nov},
number = {11},
pages = {745--756},
pmid = {25315388},
title = {{Expectation in perceptual decision making: neural and computational mechanisms}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25315388 http://www.nature.com/articles/nrn3838},
volume = {15},
year = {2014}
}
@article{Rauss2013,
abstract = {Everyone knows what bottom-up is, and how it is different from top-down. At least one is tempted to think so, given that both terms are ubiquitously used, but only rarely defined in the psychology and neuroscience literature. In this review, we highlight the problems and limitations of our current understanding of bottom-up and top-down processes, and we propose a reformulation of this distinction in terms of predictive coding.},
author = {Rauss, Karsten and Pourtois, Gilles},
doi = {10.3389/fpsyg.2013.00276},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {V1,bottom-up,predictive coding,top-down,vision},
pages = {276},
pmid = {23730295},
title = {{What is Bottom-Up and What is Top-Down in Predictive Coding?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23730295 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3656342 http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00276/abstract},
volume = {4},
year = {2013}
}
@article{Kogo2015a,
author = {Kogo, Naoki and Trengove, Chris},
doi = {10.3389/fncom.2015.00111},
issn = {1662-5188},
journal = {Frontiers in computational neuroscience},
keywords = {bayesian models,error signals,feedback,generative model,neuroanatomy,physiological,predictive coding,visual cortex},
pages = {111},
pmid = {26441621},
publisher = {Frontiers Media SA},
title = {{Is predictive coding theory articulated enough to be testable?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26441621 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4561670},
volume = {9},
year = {2015}
}
@article{VanSchalkwyk2017,
author = {van Schalkwyk, Gerrit I. and Volkmar, Fred R. and Corlett, Philip R.},
doi = {10.1007/s10803-017-3065-9},
issn = {0162-3257},
journal = {Journal of Autism and Developmental Disorders},
month = {may},
number = {5},
pages = {1323--1340},
publisher = {Springer US},
title = {{A Predictive Coding Account of Psychotic Symptoms in Autism Spectrum Disorder}},
url = {http://link.springer.com/10.1007/s10803-017-3065-9},
volume = {47},
year = {2017}
}
@phdthesis{Lippl2018,
abstract = {Although first introduced as a biological model of the brain, neural networks have found an important application as predictive algorithms in Machine Learning, most notably if they learn their parameters by gradient descent using the backpropagation algorithm. Its success has led to the discussion whether some form of backpropagation may be implemented in the brain. In this Bachelor's Thesis, I introduce a general framework of neural networks, in which I define the backpropagation algorithm and a model of neuronal computation that is called predictive coding. I prove conditions under which the parameters that are found by predictive coding converge to the ones that the backpropagation algorithm yields and discuss biological plausibility of both algorithms. I go on by studying a simple linear predictive coding model in more depth. This may be implemented with or without priors. I prove that predictive coding with priors that have increasingly high variance is equivalent to implementing predictive coding without priors.},
author = {Lippl, Samuel},
file = {:home/sflippl/Documents/Literature/Papers/Lippl - 2018 - A mathematical analysis of backpropagation and predictive coding.pdf:pdf},
school = {Ludwig Maximilians University Munich},
title = {{A mathematical analysis of backpropagation and predictive coding}},
type = {Bachelor's Thesis},
url = {https://github.com/sflippl/neural-networks/blob/master/A Mathematical Analysis of Backpropagation and Predictive Coding.pdf},
year = {2018}
}
@article{Brette2018,
abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable. Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming a structured internal model. Neural codes are inadequate for this purpose because they are unstructured. Third, coding variables are observables tied to the temporality of experiments, while spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the informational requirements of cognition.},
author = {Brette, Romain},
doi = {10.1101/168237},
journal = {bioRxiv},
month = {jul},
pages = {168237},
publisher = {Cold Spring Harbor Laboratory},
title = {{Is coding a relevant metaphor for the brain?}},
url = {https://www.biorxiv.org/content/early/2018/07/13/168237},
year = {2018}
}
@article{Gilbert2013a,
abstract = {Re-entrant or feedback pathways between cortical areas carry rich and varied information about behavioural context, including attention, expectation, perceptual tasks, working memory and motor commands. Neurons receiving such inputs effectively function as adaptive processors that are able to assume different functional states according to the task being executed. Recent data suggest that the selection of particular inputs, representing different components of an association field, enable neurons to take on different functional roles. In this Review, we discuss the various top-down influences exerted on the visual cortical pathways and highlight the dynamic nature of the receptive field, which allows neurons to carry information that is relevant to the current perceptual demands.},
author = {Gilbert, Charles D. and Li, Wu},
doi = {10.1038/nrn3476},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {may},
number = {5},
pages = {350--363},
pmid = {23595013},
title = {{Top-down influences on visual processing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23595013 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3864796 http://www.nature.com/articles/nrn3476},
volume = {14},
year = {2013}
}
@article{Crick1989,
author = {Crick, Francis},
doi = {10.1038/337129a0},
issn = {0028-0836},
journal = {Nature},
month = {jan},
number = {6203},
pages = {129--132},
title = {{The recent excitement about neural networks}},
url = {http://www.nature.com/doifinder/10.1038/337129a0},
volume = {337},
year = {1989}
}
@article{Kirkpatrick2016,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
archivePrefix = {arXiv},
arxivId = {1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
eprint = {1612.00796},
file = {:home/sflippl/Documents/Literature/Papers/Kirkpatrick et al. - 2016 - Overcoming catastrophic forgetting in neural networks.pdf:pdf},
month = {dec},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
year = {2016}
}
@article{Malinowski2018,
abstract = {Visual QA is a pivotal challenge for higher-level reasoning, requiring understanding language, vision, and relationships between many objects in a scene. Although datasets like CLEVR are designed to be unsolvable without such complex relational reasoning, some surprisingly simple feed-forward, "holistic" models have recently shown strong performance on this dataset. These models lack any kind of explicit iterative, symbolic reasoning procedure, which are hypothesized to be necessary for counting objects, narrowing down the set of relevant objects based on several attributes, etc. The reason for this strong performance is poorly understood. Hence, our work analyzes such models, and finds that minor architectural elements are crucial to performance. In particular, we find that $\backslash$textit{\{}early fusion{\}} of language and vision provides large performance improvements. This contrasts with the late fusion approaches popular at the dawn of Visual QA. We propose a simple module we call Multimodal Core, which we hypothesize performs the fundamental operations for multimodal tasks. We believe that understanding why these elements are so important to complex question answering will aid the design of better-performing algorithms for Visual QA while minimizing hand-engineering effort.},
archivePrefix = {arXiv},
arxivId = {1809.04482},
author = {Malinowski, Mateusz and Doersch, Carl},
eprint = {1809.04482},
file = {::},
month = {sep},
title = {{The Visual QA Devil in the Details: The Impact of Early Fusion and Batch Norm on CLEVR}},
url = {http://arxiv.org/abs/1809.04482},
year = {2018}
}
@article{Lansdell2018,
abstract = {Neural plasticity can be seen as ultimately aiming at the maximization of reward. However, the world is complicated and nonlinear and so are neurons' firing properties. A neuron learning to make changes that lead to the maximization of reward is an estimation problem: would there be more reward if the neural activity had been different? Statistically, this is a causal inference problem. Here we show how the spiking discontinuity of neurons can be a tool to estimate the causal influence of a neuron's activity on reward. We show how it can be used to derive a novel learning rule that can operate in the presence of non-linearities and the confounding influence of other neurons. We establish a link between simple learning rules and an existing causal inference method from econometrics, yielding proofs of both the correctness of the approach as well as its asymptotic behavior.},
author = {Lansdell, Benjamin James and Kording, Konrad Paul},
doi = {10.1101/253351},
file = {::},
journal = {bioRxiv},
month = {oct},
pages = {253351},
publisher = {Cold Spring Harbor Laboratory},
title = {{Spiking allows neurons to estimate their causal effect}},
url = {https://www.biorxiv.org/content/early/2018/10/31/253351?{\%}3Fcollection=},
year = {2018}
}
@article{Sacramento2018,
abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation - appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
archivePrefix = {arXiv},
arxivId = {1810.11393},
author = {Sacramento, Jo{\~{a}}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
eprint = {1810.11393},
file = {:home/sflippl/Documents/Literature/Papers/Sacramento et al. - 2018 - Dendritic cortical microcircuits approximate the backpropagation algorithm.pdf:pdf},
month = {oct},
title = {{Dendritic cortical microcircuits approximate the backpropagation algorithm}},
url = {http://arxiv.org/abs/1810.11393},
year = {2018}
}
@article{Enikolopov2018,
abstract = {Studies of cerebellum-like circuits in fish have demonstrated that synaptic plasticity shapes the motor corollary discharge responses of granule cells into highly-specific predictions of self-generated sensory input. However, the functional significance of such predictions, known as negative images, has not been directly tested. Here we provide evidence for improvements in neural coding and behavioral detection of prey-like stimuli due to negative images. In addition, we find that manipulating synaptic plasticity leads to specific changes in circuit output that disrupt neural coding and detection of prey-like stimuli. These results link synaptic plasticity, neural coding, and behavior and also provide a circuit-level account of how combining external sensory input with internally generated predictions enhances sensory processing.},
author = {Enikolopov, Armen G and Abbott, L F and Sawtell, Nathaniel B},
doi = {10.1016/j.neuron.2018.06.006},
file = {::},
issn = {1097-4199},
journal = {Neuron},
keywords = {cerebellum,corollary discharge,electric fish,electrosensory,synaptic plasticity},
month = {jul},
number = {1},
pages = {135--146.e3},
pmid = {30001507},
publisher = {Elsevier},
title = {{Internally Generated Predictions Enhance Neural and Behavioral Detection of Sensory Stimuli in an Electric Fish.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30001507},
volume = {99},
year = {2018}
}
@article{Bastos2012a,
abstract = {This Perspective considers the influential notion of a canonical (cortical) microcircuit in light of recent theories about neuronal processing. Specifically, we conciliate quantitative studies of microcircuitry and the functional logic of neuronal computations. We revisit the established idea that message passing among hierarchical cortical areas implements a form of Bayesian inference-paying careful attention to the implications for intrinsic connections among neuronal populations. By deriving canonical forms for these computations, one can associate specific neuronal populations with specific computational roles. This analysis discloses a remarkable correspondence between the microcircuitry of the cortical column and the connectivity implied by predictive coding. Furthermore, it provides some intuitive insights into the functional asymmetries between feedforward and feedback connections and the characteristic frequencies over which they operate.},
author = {Bastos, Andre M and Usrey, W Martin and Adams, Rick A and Mangun, George R and Fries, Pascal and Friston, Karl J},
doi = {10.1016/j.neuron.2012.10.038},
issn = {1097-4199},
journal = {Neuron},
month = {nov},
number = {4},
pages = {695--711},
pmid = {23177956},
publisher = {Elsevier},
title = {{Canonical microcircuits for predictive coding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23177956 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3777738},
volume = {76},
year = {2012}
}
@article{Najafi2018,
abstract = {{\textless}p{\textgreater}Major changes are underway in the field of perceptual decision-making. Single-neuron studies have given way to population recordings with identified cell types, traditional analyses have been extended to accommodate these large and diverse collections of neurons, and novel methods of neural disruption have provided insights about causal circuits. Further, the field has expanded to include multiple new species: rodents and invertebrates, for example, have been instrumental in demonstrating the importance of internal state on neural responses. Finally, a renewed interest in ethological stimuli prompted development of new behaviors, frequently analyzed by new, automated movement tracking methods. Taken together, these advances constitute a seismic shift in both our approach and understanding of how incoming sensory signals are used to guide decisions.{\textless}/p{\textgreater}},
author = {Najafi, Farzaneh and Churchland, Anne K.},
doi = {10.1016/j.neuron.2018.10.017},
issn = {08966273},
journal = {Neuron},
keywords = {cognition,computational models,decision-making,imaging,mice,neural data analysis},
month = {oct},
number = {2},
pages = {453--462},
publisher = {Elsevier},
title = {{Perceptual Decision-Making: A Field in the Midst of a Transformation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731830905X},
volume = {100},
year = {2018}
}
@article{Keller2018,
abstract = {{\textless}p{\textgreater}This perspective describes predictive processing as a computational framework for understanding cortical function in the context of emerging evidence, with a focus on sensory processing. We discuss how the predictive processing framework may be implemented at the level of cortical circuits and how its implementation could be falsified experimentally. Lastly, we summarize the general implications of predictive processing on cortical function in healthy and diseased states.{\textless}/p{\textgreater}},
author = {Keller, Georg B. and Mrsic-Flogel, Thomas D.},
doi = {10.1016/j.neuron.2018.10.003},
file = {:home/sflippl/Documents/Literature/Papers/Keller, Mrsic-Flogel - 2018 - Predictive Processing A Canonical Cortical Computation.pdf:pdf},
issn = {08966273},
journal = {Neuron},
keywords = {canonical microcircuit,cortex,predictive coding,predictive processing,sensory processing},
month = {oct},
number = {2},
pages = {424--435},
publisher = {Elsevier},
title = {{Predictive Processing: A Canonical Cortical Computation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318308572},
volume = {100},
year = {2018}
}
@article{Engel2001,
abstract = {Dynamic predictions: Oscillations and synchrony in top–down processing},
author = {Engel, Andreas K. and Fries, Pascal and Singer, Wolf},
doi = {10.1038/35094565},
file = {::},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {oct},
number = {10},
pages = {704--716},
publisher = {Nature Publishing Group},
title = {{Dynamic predictions: Oscillations and synchrony in top–down processing}},
url = {http://www.nature.com/articles/35094565},
volume = {2},
year = {2001}
}
@article{Stefanics2018,
abstract = {Predictive coding (PC) theory posits that our brain employs a predictive model of the environment to infer the causes of its sensory inputs. A fundamental but untested prediction of this theory is that the same stimulus should elicit distinct precision weighted prediction errors (pwPEs) when different (feature-specific) predictions are violated, even in the absence of attention. Here, we tested this hypothesis using functional magnetic resonance imaging (fMRI) and a multi-feature roving visual mismatch paradigm where rare changes in either color (red, green), or emotional expression (happy, fearful) of faces elicited pwPE responses in human participants. Using a computational model for learning and inference, we simulated pwPE trajectories of a Bayes-optimal observer and used these to analyze changes in blood oxygen level dependent (BOLD) responses to changes in color and emotional expression of faces while participants engaged in a distractor task. Controlling for visual attention by eye-tracking, we found pwPE responses to unexpected color changes in the fusiform gyrus. Conversely, unexpected changes of facial emotions elicited pwPE responses in thalamo-cortico-cerebellar structures associated with emotion processing. Our results support a general role of PC across perception, from low-level to complex and socially relevant object features, and suggest that monitoring of the social environment occurs continuously and automatically, even in the absence of attention.},
author = {Stefanics, Gabor and Stephan, Klaas Enno and Heinzle, Jakob},
doi = {10.1101/447243},
file = {::},
journal = {bioRxiv},
month = {oct},
pages = {447243},
publisher = {Cold Spring Harbor Laboratory},
title = {{Feature-specific prediction errors for visual mismatch support predictive coding accounts of perception}},
url = {https://www.biorxiv.org/content/early/2018/10/22/447243},
year = {2018}
}
@article{Fries2005,
abstract = {At any one moment, many neuronal groups in our brain are active. Microelectrode recordings have characterized the activation of single neurons and fMRI has unveiled brain-wide activation patterns. Now it is time to understand how the many active neuronal groups interact with each other and how their communication is flexibly modulated to bring about our cognitive dynamics. I hypothesize that neuronal communication is mechanistically subserved by neuronal coherence. Activated neuronal groups oscillate and thereby undergo rhythmic excitability fluctuations that produce temporal windows for communication. Only coherently oscillating neuronal groups can interact effectively, because their communication windows for input and for output are open at the same times. Thus, a flexible pattern of coherence defines a flexible communication structure, which subserves our cognitive flexibility.},
author = {Fries, Pascal},
doi = {10.1016/J.TICS.2005.08.011},
file = {:home/sflippl/Documents/Literature/Papers/Fries - 2005 - A mechanism for cognitive dynamics neuronal communication through neuronal coherence.pdf:pdf},
issn = {1364-6613},
journal = {Trends in Cognitive Sciences},
month = {oct},
number = {10},
pages = {474--480},
publisher = {Elsevier Current Trends},
title = {{A mechanism for cognitive dynamics: neuronal communication through neuronal coherence}},
url = {https://www.sciencedirect.com/science/article/pii/S1364661305002421?via{\%}3Dihub},
volume = {9},
year = {2005}
}
@article{Mullachery2018,
archivePrefix = {arXiv},
arxivId = {1801.07710},
author = {Mullachery, Vikram and Khera, Aniruddh and Husain, Amir},
eprint = {1801.07710},
file = {:home/sflippl/Documents/Literature/Papers/Mullachery, Khera, Husain - 2018 - Bayesian Neural Networks.pdf:pdf},
journal = {CoRR},
title = {{Bayesian Neural Networks}},
url = {http://arxiv.org/abs/1801.07710},
volume = {abs/1801.0},
year = {2018}
}
@article{Zhang2011,
abstract = {Recognizing objects in cluttered scenes requires attentional mechanisms to filter out distracting information. Previous studies have found several physiological correlates of attention in visual cortex, including larger responses for attended objects. However, it has been unclear whether these attention-related changes have a large impact on information about objects at the neural population level. To address this question, we trained monkeys to covertly deploy their visual attention from a central fixation point to one of three objects displayed in the periphery, and we decoded information about the identity and position of the objects from populations of ∼200 neurons from the inferior temporal cortex using a pattern classifier. The results show that before attention was deployed, information about the identity and position of each object was greatly reduced relative to when these objects were shown in isolation. However, when a monkey attended to an object, the pattern of neural activity, represented as a vector with dimensionality equal to the size of the neural population, was restored toward the vector representing the isolated object. Despite this nearly exclusive representation of the attended object, an increase in the salience of nonattended objects caused “bottom-up” mechanisms to override these “top-down” attentional enhancements. The method described here can be used to assess which attention-related physiological changes are directly related to object recognition, and should be helpful in assessing the role of additional physiological changes in the future.},
author = {Zhang, Ying and Meyers, Ethan M and Bichot, Narcisse P and Serre, Thomas and Poggio, Tomaso A and Desimone, Robert},
file = {:home/sflippl/Documents/Literature/Papers/Zhang et al. - 2011 - Object decoding with attention in inferior temporal cortex.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
month = {may},
number = {21},
pages = {8850 LP -- 8855},
title = {{Object decoding with attention in inferior temporal cortex}},
url = {http://www.pnas.org/content/108/21/8850.abstract},
volume = {108},
year = {2011}
}
@article{doi:10.1111/ejn.12453,
abstract = {Abstract Neuronal rhythms are ubiquitous features of brain dynamics, and are highly correlated with cognitive processing. However, the relationship between the physiological mechanisms producing these rhythms and the functions associated with the rhythms remains mysterious. This article investigates the contributions of rhythms to basic cognitive computations (such as filtering signals by coherence and/or frequency) and to major cognitive functions (such as attention and multi-modal coordination). We offer support to the premise that the physiology underlying brain rhythms plays an essential role in how these rhythms facilitate some cognitive operations.},
author = {Cannon, Jonathan and McCarthy, Michelle M and Lee, Shane and Lee, Jung and B{\"{o}}rgers, Christoph and Whittington, Miles A and Kopell, Nancy},
doi = {10.1111/ejn.12453},
file = {:home/sflippl/Documents/Literature/Papers/Cannon et al. - 2014 - Neurosystems brain rhythms and cognitive processing.pdf:pdf},
journal = {European Journal of Neuroscience},
keywords = {attention,beta rhythm,coherence filtering,frequency filtering,gamma rhythm},
number = {5},
pages = {705--719},
title = {{Neurosystems: brain rhythms and cognitive processing}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ejn.12453},
volume = {39},
year = {2014}
}
@article{Bosman2012,
abstract = {A central motif in neuronal networks is convergence, linking several input neurons to one target neuron. In visual cortex, convergence renders target neurons responsive to complex stimuli. Yet, convergence typically sends multiple stimuli to a target, and the behaviorally relevant stimulus must be selected. We used two stimuli, activating separate electrocorticographic V1 sites, and both activating an electrocorticographic V4 site equally strongly. When one of those stimuli activated one V1 site, it gamma synchronized (60–80 Hz) to V4. When the two stimuli activated two V1 sites, primarily the relevant one gamma synchronized to V4. Frequency bands of gamma activities showed substantial overlap containing the band of interareal coherence. The relevant V1 site had its gamma peak frequency 2–3 Hz higher than the irrelevant V1 site and 4–6 Hz higher than V4. Gamma-mediated interareal influences were predominantly directed from V1 to V4. We propose that selective synchronization renders relevant input effective, thereby modulating effective connectivity.},
author = {Bosman, Conrado A. and Schoffelen, Jan-Mathijs and Brunet, Nicolas and Oostenveld, Robert and Bastos, Andre M. and Womelsdorf, Thilo and Rubehn, Birthe and Stieglitz, Thomas and {De Weerd}, Peter and Fries, Pascal},
doi = {10.1016/J.NEURON.2012.06.037},
file = {::},
issn = {0896-6273},
journal = {Neuron},
month = {sep},
number = {5},
pages = {875--888},
publisher = {Cell Press},
title = {{Attentional Stimulus Selection through Selective Synchronization between Monkey Visual Areas}},
url = {https://www.sciencedirect.com/science/article/pii/S089662731200623X},
volume = {75},
year = {2012}
}
@article{Reynolds2009,
abstract = {Attention has been found to have a wide variety of effects on the responses of neurons in visual cortex. We describe a model of attention that exhibits each of these different forms of attentional modulation, depending on the stimulus conditions and the spread (or selectivity) of the attention field in the model. The model helps reconcile proposals that have been taken to represent alternative theories of attention. We argue that the variety and complexity of the results reported in the literature emerge from the variety of empirical protocols that were used, such that the results observed in any one experiment depended on the stimulus conditions and the subject's attentional strategy, a notion that we define precisely in terms of the attention field in the model, but that has not typically been completely under experimental control.},
author = {Reynolds, John H. and Heeger, David J.},
doi = {10.1016/J.NEURON.2009.01.002},
file = {::},
issn = {0896-6273},
journal = {Neuron},
month = {jan},
number = {2},
pages = {168--185},
publisher = {Cell Press},
title = {{The Normalization Model of Attention}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627309000038},
volume = {61},
year = {2009}
}
@article{Markov2014,
abstract = {ABSTRACT The laminar location of the cell bodies and terminals of interareal connections determines the hierarchical structural organization of the cortex and has been intensively studied. However, we still have only a rudimentary understanding of the connectional principles of feedforward (FF) and feedback (FB) pathways. Quantitative analysis of retrograde tracers was used to extend the notion that the laminar distribution of neurons interconnecting visual areas provides an index of hierarchical distance (percentage of supragranular labeled neurons [SLN]). We show that: 1) SLN values constrain models of cortical hierarchy, revealing previously unsuspected areal relations; 2) SLN reflects the operation of a combinatorial distance rule acting differentially on sets of connections between areas; 3) Supragranular layers contain highly segregated bottom-up and top-down streams, both of which exhibit point-to-point connectivity. This contrasts with the infragranular layers, which contain diffuse bottom-up and top-down streams; 4) Cell filling of the parent neurons of FF and FB pathways provides further evidence of compartmentalization; 5) FF pathways have higher weights, cross fewer hierarchical levels, and are less numerous than FB pathways. Taken together, the present results suggest that cortical hierarchies are built from supra- and infragranular counterstreams. This compartmentalized dual counterstream organization allows point-to-point connectivity in both bottom-up and top-down directions. J. Comp. Neurol. 522:225–259, 2014. {\textcopyright} 2013 Wiley Periodicals, Inc.},
annote = {This might also be relevant to predictive coding in general. The coherence of gamma oscillation delays throughout the granular layers with the respective feedforward and feedback connections seem to be quite interesting.},
author = {Markov, Nikola T and Vezoli, Julien and Chameau, Pascal and Falchier, Arnaud and Quilodran, Ren{\'{e}} and Huissoud, Cyril and Lamy, Camille and Misery, Pierre and Giroud, Pascale and Ullman, Shimon and Barone, Pascal and Dehay, Colette and Knoblauch, Kenneth and Kennedy, Henry},
doi = {10.1002/cne.23458},
file = {:home/sflippl/Documents/Literature/Papers/Markov et al. - 2014 - Anatomy of hierarchy Feedforward and feedback pathways in macaque visual cortex.pdf:pdf},
journal = {Journal of Comparative Neurology},
keywords = {cell morphology,monkey,neocortex,retrograde tracing},
number = {1},
pages = {225--259},
title = {{Anatomy of hierarchy: Feedforward and feedback pathways in macaque visual cortex}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.23458},
volume = {522},
year = {2014}
}
@article{VanKerkoerle2014,
abstract = {One of the main unresolved questions in cognitive neuroscience is how low-level and high-level areas of the visual cortex interact with each other during perception and cognition. We investigated whether cortical oscillations can be used to distinguish feedforward from feedback processing. We studied the propagation of $\alpha$- and $\gamma$-oscillations through the cortical layers and between different visual cortical areas. We induced oscillations in different areas with microstimulation and influenced them using a pharmacological approach. The results of these experiments demonstrate that $\gamma$-oscillations propagate in the feedforward direction, whereas $\alpha$-oscillations propagate in the feedback direction. We conclude that high- and low-frequency oscillations provide markers of feedforward and feedback processing, respectively.Cognitive functions rely on the coordinated activity of neurons in many brain regions, but the interactions between cortical areas are not yet well understood. Here we investigated whether low-frequency ($\alpha$) and high-frequency ($\gamma$) oscillations characterize different directions of information flow in monkey visual cortex. We recorded from all layers of the primary visual cortex (V1) and found that $\gamma$-waves are initiated in input layer 4 and propagate to the deep and superficial layers of cortex, whereas $\alpha$-waves propagate in the opposite direction. Simultaneous recordings from V1 and downstream area V4 confirmed that $\gamma$- and $\alpha$-waves propagate in the feedforward and feedback direction, respectively. Microstimulation in V1 elicited $\gamma$-oscillations in V4, whereas microstimulation in V4 elicited $\alpha$-oscillations in V1, thus providing causal evidence for the opposite propagation of these rhythms. Furthermore, blocking NMDA receptors, thought to be involved in feedback processing, suppressed $\alpha$ while boosting $\gamma$. These results provide new insights into the relation between brain rhythms and cognition.},
author = {van Kerkoerle, Timo and Self, Matthew W and Dagnino, Bruno and Gariel-Mathis, Marie-Alice and Poort, Jasper and van der Togt, Chris and Roelfsema, Pieter R},
file = {:home/sflippl/Documents/Literature/Papers/van Kerkoerle et al. - 2014 - Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
month = {oct},
number = {40},
pages = {14332 LP -- 14341},
title = {{Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex}},
url = {http://www.pnas.org/content/111/40/14332.abstract},
volume = {111},
year = {2014}
}
@article{Ainsworth2012,
abstract = {In the CNS, activity of individual neurons has a small but quantifiable relationship to sensory representations and motor outputs. Coactivation of a few 10s to 100s of neurons can code sensory inputs and behavioral task performance within psychophysical limits. However, in a sea of sensory inputs and demand for complex motor outputs how is the activity of such small subpopulations of neurons organized? Two theories dominate in this respect: increases in spike rate (rate coding) and sharpening of the coincidence of spiking in active neurons (temporal coding). Both have computational advantages and are far from mutually exclusive. Here, we review evidence for a bias in neuronal circuits toward temporal coding and the coexistence of rate and temporal coding during population rhythm generation. The coincident expression of multiple types of gamma rhythm in sensory cortex suggests a mechanistic substrate for combining rate and temporal codes on the basis of stimulus strength.},
author = {Ainsworth, Matt and Lee, Shane and Cunningham, Mark O. and Traub, Roger D. and Kopell, Nancy J. and Whittington, Miles A.},
doi = {10.1016/J.NEURON.2012.08.004},
file = {:home/sflippl/Documents/Literature/Papers/Ainsworth et al. - 2012 - Rates and Rhythms A Synergistic View of Frequency and Temporal Coding in Neuronal Networks.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
month = {aug},
number = {4},
pages = {572--583},
publisher = {Cell Press},
title = {{Rates and Rhythms: A Synergistic View of Frequency and Temporal Coding in Neuronal Networks}},
url = {https://www.sciencedirect.com/science/article/pii/S089662731200709X},
volume = {75},
year = {2012}
}
@article{Yamins2014,
abstract = {Humans and monkeys easily recognize objects in scenes. This ability is known to be supported by a network of hierarchically interconnected brain areas. However, understanding neurons in higher levels of this hierarchy has long remained a major challenge in visual systems neuroscience. We use computational techniques to identify a neural network model that matches human performance on challenging object categorization tasks. Although not explicitly constrained to match neural data, this model turns out to be highly predictive of neural responses in both the V4 and inferior temporal cortex, the top two layers of the ventral visual hierarchy. In addition to yielding greatly improved models of visual cortex, these results suggest that a process of biological performance optimization directly shaped neural mechanisms.The ventral visual stream underlies key human visual object recognition abilities. However, neural encoding in the higher areas of the ventral stream remains poorly understood. Here, we describe a modeling approach that yields a quantitatively accurate model of inferior temporal (IT) cortex, the highest ventral cortical area. Using high-throughput computational techniques, we discovered that, within a class of biologically plausible hierarchical neural network models, there is a strong correlation between a model's categorization performance and its ability to predict individual IT neural unit response data. To pursue this idea, we then identified a high-performing neural network that matches human performance on a range of recognition tasks. Critically, even though we did not constrain this model to match neural data, its top output layer turns out to be highly predictive of IT spiking responses to complex naturalistic images at both the single site and population levels. Moreover, the model's intermediate layers are highly predictive of neural responses in the V4 cortex, a midlevel visual area that provides the dominant cortical input to IT. These results show that performance optimization—applied in a biologically appropriate model class—can be used to build quantitative predictive models of neural processing.},
author = {Yamins, Daniel L K and Hong, Ha and Cadieu, Charles F and Solomon, Ethan A and Seibert, Darren and DiCarlo, James J},
file = {:home/sflippl/Documents/Literature/Papers/Yamins et al. - 2014 - Performance-optimized hierarchical models predict neural responses in higher visual cortex.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
month = {jun},
number = {23},
pages = {8619 LP -- 8624},
title = {{Performance-optimized hierarchical models predict neural responses in higher visual cortex}},
url = {http://www.pnas.org/content/111/23/8619.abstract},
volume = {111},
year = {2014}
}
@article{Alexander2018,
abstract = {Cognitive control and decision-making relies on the interplay of medial and lateral prefrontal cortex (mPFC/LPFC), particularly for circumstances in which correct behavior requires integrating and selecting among multiple sources of interrelated information. While the interaction between mPFC and LPFC is generally acknowledged as a crucial circuit in adaptive behavior, the nature of this interaction remains open to debate, with various proposals suggesting complementary roles in (i) signaling the need for and implementing control, (ii) identifying and selecting appropriate behavioral policies from a candidate set, and (iii) constructing behavioral schemata for performance of structured tasks. Although these proposed roles capture salient aspects of conjoint mPFC/LPFC function, none are sufficiently well-specified to provide a detailed account of the continuous interaction of the two regions during ongoing behavior. A recent computational model of mPFC and LPFC, the Hierarchical Error Representation (HER) model, places the regions within the framework of hierarchical predictive coding, and suggests how they interact during behavioral periods preceding and following salient events. In this manuscript, we extend the HER model to incorporate real-time temporal dynamics and demonstrate how the extended model is able to capture single-unit neurophysiological, behavioral, and network effects previously reported in the literature. Our results add to the wide range of results that can be accounted for by the HER model, and provide further evidence for predictive coding as a unifying framework for understanding PFC function and organization.},
author = {Alexander, William H and Womelsdorf, Thilo},
doi = {10.1101/439927},
journal = {bioRxiv},
month = {oct},
pages = {439927},
publisher = {Cold Spring Harbor Laboratory},
title = {{Interactions of medial and lateral prefrontal cortex in hierarchical predictive coding}},
url = {https://www.biorxiv.org/content/early/2018/10/10/439927},
year = {2018}
}
@article{Naud2018,
abstract = {Many cortical neurons combine the information ascending and descending the cortical hierarchy. In the classical view, this information is combined nonlinearly to give rise to a single firing-rate output, which collapses all input streams into one. We analyze the extent to which neurons can simultaneously represent multiple input streams by using a code that distinguishes spike timing patterns at the level of a neural ensemble. Using computational simulations constrained by experimental data, we show that cortical neurons are well suited to generate such multiplexing. Interestingly, this neural code maximizes information for short and sparse bursts, a regime consistent with in vivo recordings. Neurons can also demultiplex this information, using specific connectivity patterns. The anatomy of the adult mammalian cortex suggests that these connectivity patterns are used by the nervous system to maintain sparse bursting and optimal multiplexing. Contrary to firing-rate coding, our findings indicate that the physiology and anatomy of the cortex may be interpreted as optimizing the transmission of multiple independent signals to different targets.},
author = {Naud, Richard and Sprekeler, Henning},
doi = {10.1073/pnas.1720995115},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
keywords = {cerebral cortex,dendritic computation,multiplexing,neural coding,short-term plasticity},
month = {jul},
number = {27},
pages = {E6329--E6338},
pmid = {29934400},
title = {{Sparse bursts optimize information transmission in a multiplexed neural code}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29934400 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC6142200 http://www.pnas.org/lookup/doi/10.1073/pnas.1720995115},
volume = {115},
year = {2018}
}
@article{Ji2018,
author = {Ji, Jie Lisa and Spronk, Marjolein and Kulkarni, Kaustubh and Repov{\v{s}}, Grega and Anticevic, Alan and Cole, Michael W.},
doi = {10.1016/j.neuroimage.2018.10.006},
issn = {10538119},
journal = {NeuroImage},
month = {oct},
title = {{Mapping the human brain's cortical-subcortical functional network organization}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918319657},
year = {2018}
}
@article{Mante2013,
abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V and Newsome, William T},
doi = {10.1038/nature12742},
issn = {1476-4687},
journal = {Nature},
month = {nov},
number = {7474},
pages = {78--84},
pmid = {24201281},
title = {{Context-dependent computation by recurrent dynamics in prefrontal cortex.}},
url = {http://www.nature.com/articles/nature12742 http://www.ncbi.nlm.nih.gov/pubmed/24201281 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4121670},
volume = {503},
year = {2013}
}
@article{Liu2018,
abstract = {Rapid internal representations are continuously formed based on single experiential episodes in space and time, but the neuronal ensemble mechanisms enabling rapid encoding without constraining the capacity for multiple distinct representations are unknown. We developed a probabilistic statistical model of hippocampal spontaneous sequential activity and revealed existence of an internal model of generative predictive codes for the regularities of multiple future novel spatial sequences. During navigation, the inferred difference between external stimuli and the internal model was encoded by emergence of intrinsic-unlikely, novel functional connections, which updated the model by preferentially potentiating post-experience. This internal model and these predictive codes depended on neuronal organization into inferred modules of short, high-repeat sequential neuronal “tuplets” operating as “neuro-codons.” We propose that flexible multiplexing of neuronal tuplets into repertoires of extended sequences vastly expands the capacity of hippocampal predictive codes, which could initiate top-down hierarchical cortical loops for spatial and mental navigation and rapid learning.},
author = {Liu, Kefei and Sibille, Jeremie and Dragoi, George},
doi = {10.1016/J.NEURON.2018.07.047},
issn = {0896-6273},
journal = {Neuron},
month = {sep},
number = {6},
pages = {1329--1341.e6},
publisher = {Cell Press},
title = {{Generative Predictive Codes by Multiplexed Hippocampal Neuronal Tuplets}},
url = {https://ezproxy-prd.bodleian.ox.ac.uk:2073/science/article/pii/S0896627318306457},
volume = {99},
year = {2018}
}
@article{Han2018,
abstract = {Tracing of projection neuron axons from the primary visual cortex to their targets shows that these neurons often project to multiple cortical areas of the mouse brain.},
author = {Han, Yunyun and Kebschull, Justus M. and Campbell, Robert A. A. and Cowan, Devon and Imhof, Fabia and Zador, Anthony M. and Mrsic-Flogel, Thomas D.},
doi = {10.1038/nature26159},
file = {::},
issn = {0028-0836},
journal = {Nature},
keywords = {Extrastriate cortex,Neural circuits,Sensory processing},
month = {mar},
number = {7699},
pages = {51--56},
publisher = {Nature Publishing Group},
title = {{The logic of single-cell projections from visual cortex}},
url = {http://www.nature.com/doifinder/10.1038/nature26159},
volume = {556},
year = {2018}
}
@article{Fries2015,
abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30–90 Hz) synchronization modulates excitation rapidly enough that it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g., representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up-directed gamma-band influences are controlled by predominantly top-down-directed alpha-beta-band (8–20 Hz) influences. Attention itself samples stimuli at a 7–8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise, and selective.},
author = {Fries, Pascal},
doi = {10.1016/J.NEURON.2015.09.034},
file = {:home/sflippl/Documents/Literature/Papers/Fries - 2015 - Rhythms for Cognition Communication through Coherence.pdf:pdf},
issn = {0896-6273},
journal = {Neuron},
month = {oct},
number = {1},
pages = {220--235},
publisher = {Cell Press},
title = {{Rhythms for Cognition: Communication through Coherence}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627315008235},
volume = {88},
year = {2015}
}
@article{Zhang2014,
author = {Zhang, S. and Xu, M. and Kamigaki, T. and {Hoang Do}, J. P. and Chang, W.-C. and Jenvay, S. and Miyamichi, K. and Luo, L. and Dan, Y.},
doi = {10.1126/science.1254126},
issn = {0036-8075},
journal = {Science},
month = {aug},
number = {6197},
pages = {660--665},
title = {{Long-range and local circuits for top-down modulation of visual cortex processing}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1254126},
volume = {345},
year = {2014}
}
@article{Friston2010,
author = {Friston, Karl},
doi = {10.1038/nrn2787},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {feb},
number = {2},
pages = {127--138},
title = {{The free-energy principle: a unified brain theory?}},
url = {http://www.nature.com/articles/nrn2787},
volume = {11},
year = {2010}
}
@article{Hinton2007,
author = {Hinton, Geoffrey E.},
doi = {10.1016/j.tics.2007.09.004},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
month = {oct},
number = {10},
pages = {428--434},
title = {{Learning multiple layers of representation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661307002173},
volume = {11},
year = {2007}
}
@article{Gilbert2013,
author = {Gilbert, Charles D. and Li, Wu},
doi = {10.1038/nrn3476},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {may},
number = {5},
pages = {350--363},
title = {{Top-down influences on visual processing}},
url = {http://www.nature.com/articles/nrn3476},
volume = {14},
year = {2013}
}
@article{Harris2013,
author = {Harris, Kenneth D. and Mrsic-Flogel, Thomas D.},
doi = {10.1038/nature12654},
issn = {0028-0836},
journal = {Nature},
month = {nov},
number = {7474},
pages = {51--58},
title = {{Cortical connectivity and sensory coding}},
url = {http://www.nature.com/articles/nature12654},
volume = {503},
year = {2013}
}
@article{Makino2015,
abstract = {By examining the activity of layer 2/3 excitatory neurons in the mouse primary visual cortex, the authors demonstrate that learning enhances the relative impact of top-down processing by the retrosplenial cortex while reducing the influence of the bottom-up stream from layer 4 excitatory neurons. This effect is partially mediated by somatostatin-expressing inhibitory neurons.},
author = {Makino, Hiroshi and Komiyama, Takaki},
doi = {10.1038/nn.4061},
file = {::},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Cortex,Sensory processing,Striate cortex},
month = {aug},
number = {8},
pages = {1116--1122},
publisher = {Nature Publishing Group},
title = {{Learning enhances the relative impact of top-down processing in the visual cortex}},
url = {http://www.nature.com/articles/nn.4061},
volume = {18},
year = {2015}
}
@article{,
doi = {10.1016/J.NEURON.2015.05.037},
issn = {0896-6273},
journal = {Neuron},
month = {jun},
number = {6},
pages = {1478--1490},
publisher = {Cell Press},
title = {{Learning Enhances Sensory and Multiple Non-sensory Representations in Primary Visual Cortex}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627315004766},
volume = {86},
year = {2015}
}
@article{,
doi = {10.1016/J.NEURON.2018.07.046},
issn = {0896-6273},
journal = {Neuron},
month = {sep},
number = {5},
pages = {1040--1054.e5},
publisher = {Cell Press},
title = {{Mouse Motor Cortex Coordinates the Behavioral Response to Unpredicted Sensory Feedback}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627318306445},
volume = {99},
year = {2018}
}
@article{Liebscher2016,
abstract = {Neurodegenerative processes in Alzheimer's disease (AD) affect the structure and function of neurons [1–4], resulting in altered neuronal activity patterns comprising neuronal hypo- and hyperactivity [5, 6] and causing the disruption of long-range projections [7, 8]. Impaired information processing between functionally connected brain areas is evident in defective visuomotor integration, an early sign of the disease [9–11]. The cellular and neuronal circuit mechanisms underlying this disruption of information processing in AD, however, remain elusive. Recent studies in mice suggest that visuomotor integration already occurs in primary visual cortex (V1), as it not only processes sensory input but also exhibits strong motor-related activity, likely driven by neuromodulatory or excitatory inputs [12–17]. Here, we probed the integration of visual—and motor-related—inputs in V1 of behaving APP/PS1 [18] mice, a well-characterized mouse model of AD, using two-photon calcium imaging. We find that sensorimotor signals in APP/PS1 mice are differentially affected: while visually driven and motor-related signals are strongly reduced, neuronal responses signaling a mismatch between expected and actual visual flow are selectively spared. We furthermore observe an increase in aberrant activity during quiescent states in APP/PS1 mice. Jointly, the reduction in running-correlated activity and the enhanced aberrant activity degrade the coding accuracy of the network, indicating that the impairment of visuomotor integration in AD is already taking place at early stages of visual processing.},
author = {Liebscher, Sabine and Keller, Georg B. and Goltstein, Pieter M. and Bonhoeffer, Tobias and H{\"{u}}bener, Mark},
doi = {10.1016/J.CUB.2016.01.070},
file = {::},
issn = {0960-9822},
journal = {Current Biology},
month = {apr},
number = {7},
pages = {956--964},
publisher = {Cell Press},
title = {{Selective Persistence of Sensorimotor Mismatch Signals in Visual Cortex of Behaving Alzheimer's Disease Mice}},
url = {https://www.sciencedirect.com/science/article/pii/S0960982216300173},
volume = {26},
year = {2016}
}
@article{,
doi = {10.1016/J.NEURON.2017.08.036},
issn = {0896-6273},
journal = {Neuron},
month = {sep},
number = {6},
pages = {1420--1432.e5},
publisher = {Cell Press},
title = {{A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317307791},
volume = {95},
year = {2017}
}
@article{Stachenfeld2017,
author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
doi = {10.1038/nn.4650},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {oct},
publisher = {Nature Research},
title = {{The hippocampus as a predictive map}},
url = {http://www.nature.com/doifinder/10.1038/nn.4650},
year = {2017}
}
@article{Zhuang2017,
abstract = {Visual information in the visual cortex is processed in a hierarchical manner. Recent studies show that higher visual areas, such as V2, V3, and V4, respond more vigorously to images with naturalistic higher-order statistics than to images lacking them. This property is a functional signature of higher areas, as it is much weaker or even absent in the primary visual cortex (V1). However, the mechanism underlying this signature remains elusive. We studied this problem using computational models. In several typical hierarchical visual models including the AlexNet, VggNet and SHMAX, this signature was found to be prominent in higher layers but much weaker in lower layers. By changing both the model structure and experimental settings, we found that the signature strongly correlated with sparse firing of units in higher layers but not with any other factors, including model structure, training algorithm (supervised or unsupervised), receptive field size, and property of training stimuli. The results suggest an important role of sparse neuronal activity underlying this special feature of higher visual areas.},
author = {Zhuang, Chengxu and Wang, Yulong and Yamins, Daniel and Hu, Xiaolin},
doi = {10.3389/fncom.2017.00100},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
keywords = {Higher-order statistics,V1,V2,V4,Visual Processing,deep learning},
month = {oct},
pages = {100},
publisher = {Frontiers},
title = {{Deep Learning Predicts Correlation between a Functional Signature of Higher Visual Areas and Sparse Firing of Neurons}},
url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00100/full},
volume = {11},
year = {2017}
}
@article{,
doi = {10.1016/J.NEURON.2017.08.036},
issn = {0896-6273},
journal = {Neuron},
month = {sep},
number = {6},
pages = {1420--1432.e5},
publisher = {Cell Press},
title = {{A Sensorimotor Circuit in Mouse Cortex for Visual Flow Predictions}},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317307791{\#}fig3},
volume = {95},
year = {2017}
}
@article{Fiser2016,
abstract = {The authors find that activity in rodent visual cortex can depend on the animal's location in a virtual environment and can predict upcoming visual stimuli. Omitting a stimulus that a mouse expects to see results in a strong mismatch signal, implying that visual cortex compares visual signals to expectations in familiar environments.},
author = {Fiser, Aris and Mahringer, David and Oyibo, Hassana K and Petersen, Anders V and Leinweber, Marcus and Keller, Georg B},
doi = {10.1038/nn.4385},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Learning and memory,Sensory processing},
month = {sep},
number = {12},
pages = {1658--1664},
publisher = {Nature Publishing Group},
title = {{Experience-dependent spatial expectations in mouse visual cortex}},
url = {http://www.nature.com/doifinder/10.1038/nn.4385},
volume = {19},
year = {2016}
}
@article{McAlonan2008,
abstract = {The striking effects of shifts of attention are well illustrated by the behaviour of children with attention-deficit hyperactivity disorder. It is thought that attentional effects arise in the thalamus, but for the visual system it has proved difficult to determine precisely where the earliest stages of sensory processing take place. McAlonan et al. now demonstrate in experiments on macaque monkeys that spatial attentional modulation takes place in the lateral geniculate nucleus, and opposing effects in the adjacent thalamic reticular nucleus that makes inhibitory connections onto it. This reciprocal activity may be a mechanism for generating transient attention, in confirmation of a proposal made by Francis Crick nearly 25 years ago when his interests shifted from DNA to neuroscience. He hypothesized that a spotlight of attention was directed by a tiny nucleus in the brain, the thalamic reticular nucleus.},
author = {McAlonan, Kerry and Cavanaugh, James and Wurtz, Robert H.},
doi = {10.1038/nature07382},
issn = {0028-0836},
journal = {Nature},
month = {nov},
number = {7220},
pages = {391--394},
publisher = {Nature Publishing Group},
title = {{Guarding the gateway to cortex with attention in visual thalamus}},
url = {http://www.nature.com/doifinder/10.1038/nature07382},
volume = {456},
year = {2008}
}
@article{Bzdok2017,
author = {Bzdok, Danilo and Krzywinski, Martin and Altman, Naomi},
doi = {10.1038/nmeth.4526},
issn = {1548-7091},
journal = {Nature Methods},
month = {nov},
number = {12},
pages = {1119--1120},
title = {{Points of Significance: Machine learning: a primer}},
url = {http://www.nature.com/doifinder/10.1038/nmeth.4526},
volume = {14},
year = {2017}
}
@article{Jensen2014,
abstract = {Sensory systems must rely on powerful mechanisms for organizing complex information. We propose a framework in which inhibitory alpha oscillations limit and prioritize neuronal processing. At oscillatory peaks, inhibition prevents neuronal firing. As the inhibition ramps down within a cycle, a set of neuronal representations will activate sequentially according to their respective excitability. Both top-down and bottom-up drives determine excitability; in particular, spatial attention is a major top-down influence. On a shorter time scale, fast recurrent inhibition segments representations in slots 10–30ms apart, generating gamma-band activity at the population level. The proposed mechanism serves to convert spatially distributed representations in early visual regions to a temporal phase code: that is, ‘to-do lists' that can be processed sequentially by downstream regions.},
author = {Jensen, Ole and Gips, Bart and Bergmann, Til Ole and Bonnefond, Mathilde},
doi = {10.1016/J.TINS.2014.04.001},
issn = {0166-2236},
journal = {Trends in Neurosciences},
month = {jul},
number = {7},
pages = {357--369},
publisher = {Elsevier Current Trends},
title = {{Temporal coding organized by coupled alpha and gamma oscillations prioritize visual processing}},
url = {http://www.sciencedirect.com/science/article/pii/S0166223614000605?via{\%}3Dihub},
volume = {37},
year = {2014}
}
@article{Bengio2015,
abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-Timing-Dependent Plasticity) arises out of a simple update rule that makes a lot of sense from a machine learning point of view and can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
archivePrefix = {arXiv},
arxivId = {1502.04156},
author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
eprint = {1502.04156},
month = {feb},
title = {{Towards Biologically Plausible Deep Learning}},
url = {http://arxiv.org/abs/1502.04156},
year = {2015}
}
@article{Silver2010,
abstract = {Individual neurons transform the relationship between synaptic input and output firing by utilizing both linear and nonlinear mechanisms. Angus Silver discusses the various underlying biophysical mechanisms in relation to the complexity of neuronal morphology and the neural coding regimes in which they are likely to operate.},
author = {Silver, R. Angus},
doi = {10.1038/nrn2864},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
keywords = {Action potential generation,Neural encoding,Synaptic plasticity},
month = {jun},
number = {7},
pages = {474--489},
publisher = {Nature Publishing Group},
title = {{Neuronal arithmetic}},
url = {http://www.nature.com/doifinder/10.1038/nrn2864},
volume = {11},
year = {2010}
}
@article{Mante2013a,
abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
doi = {10.1038/nature12742},
issn = {0028-0836},
journal = {Nature},
month = {nov},
number = {7474},
pages = {78--84},
pmid = {24201281},
title = {{Context-dependent computation by recurrent dynamics in prefrontal cortex}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24201281 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4121670 http://www.nature.com/articles/nature12742},
volume = {503},
year = {2013}
}
@article{Chalk2017,
abstract = {A central goal in theoretical neuroscience is to predict the response properties of sensory neurons from first principles. To this end, "efficient coding" posits that sensory neurons encode maximal information about their inputs given internal constraints. There exist, however, many variants of efficient coding (e.g., redundancy reduction, different formulations of predictive coding, robust coding, sparse coding, etc.), differing in their regimes of applicability, in the relevance of signals to be encoded, and in the choice of constraints. It is unclear how these types of efficient coding relate or what is expected when different coding objectives are combined. Here we present a unified framework that encompasses previously proposed efficient coding models and extends to unique regimes. We show that optimizing neural responses to encode predictive information can lead them to either correlate or decorrelate their inputs, depending on the stimulus statistics; in contrast, at low noise, efficiently encoding the past always predicts decorrelation. Later, we investigate coding of naturalistic movies and show that qualitatively different types of visual motion tuning and levels of response sparsity are predicted, depending on whether the objective is to recover the past or predict the future. Our approach promises a way to explain the observed diversity of sensory neural responses, as due to multiple functional goals and constraints fulfilled by different cell types and/or circuits.},
author = {Chalk, Matthew and Marre, Olivier and Tka{\v{c}}ik, Ga{\v{s}}per},
doi = {10.1073/pnas.1711114115},
file = {::},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {efficient coding,information theory,neural coding,prediction,sparse coding},
month = {dec},
pages = {201711114},
pmid = {29259111},
publisher = {National Academy of Sciences},
title = {{Toward a unified theory of efficient, predictive, and sparse coding.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29259111},
year = {2017}
}
@article{Sacramento2017,
abstract = {Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning.},
archivePrefix = {arXiv},
arxivId = {1801.00062},
author = {Sacramento, Jo{\~{a}}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
eprint = {1801.00062},
month = {dec},
title = {{Dendritic error backpropagation in deep cortical microcircuits}},
url = {http://arxiv.org/abs/1801.00062},
year = {2017}
}
@article{Goldman2017,
abstract = {Neuroscience research has become increasingly reliant upon quantitative and computational data analysis and modeling techniques. However, the vast majority of neuroscientists are still trained within the traditional biology curriculum, in which computational and quantitative approaches beyond elementary statistics may be given little emphasis. Here we provide the results of an informal poll of computational and other neuroscientists that sought to identify critical needs, areas for improvement, and educational resources for computational neuroscience training. Motivated by this survey, we suggest steps to facilitate quantitative and computational training for future neuroscientists.},
author = {Goldman, Mark S and Fee, Michale S},
doi = {10.1016/J.CONB.2017.06.007},
issn = {0959-4388},
journal = {Current Opinion in Neurobiology},
month = {oct},
pages = {25--30},
publisher = {Elsevier Current Trends},
title = {{Computational training for the next generation of neuroscientists}},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817301599},
volume = {46},
year = {2017}
}
@article{CarsenStringer2018,
author = {{Carsen Stringer} and {Kenneth Harris}},
doi = {https://doi.org/10.1101/374090},
journal = {BioRxiv},
title = {{High-dimensional geometry of population responses in visual cortex}},
year = {2018}
}
@article{Akrami2018,
abstract = {A working memory task in rats demonstrates that the posterior parietal cortex is a critical locus for the representation and use of prior stimulus information.},
author = {Akrami, Athena and Kopec, Charles D. and Diamond, Mathew E. and Brody, Carlos D.},
doi = {10.1038/nature25510},
file = {::},
issn = {0028-0836},
journal = {Nature},
keywords = {Perception,Working memory},
month = {feb},
number = {7692},
pages = {368--372},
publisher = {Nature Publishing Group},
title = {{Posterior parietal cortex represents sensory history and mediates its effects on behaviour}},
url = {http://www.nature.com/doifinder/10.1038/nature25510},
volume = {554},
year = {2018}
}
@article{Goldstein2018,
author = {Goldstein},
journal = {eLife},
title = {{Point of View: Are theoretical results ‘Results'?}},
year = {2018}
}
@article{Baltieri2018,
abstract = {The assumption that action and perception can be investigated independently is entrenched in theories, models and experimental approaches across the brain and mind sciences. In cognitive science, this has been a central point of contention between computationalist and 4Es (enactive, embodied, extended and embedded) theories of cognition, with the former embracing the "classical sandwich", modular, architecture of the mind and the latter actively denying this separation can be made. In this work we suggest that the modular independence of action and perception strongly resonates with the separation principle of control theory and furthermore that this principle provides formal criteria within which to evaluate the implications of the modularity of action and perception. We will also see that real-time feedback with the environment, often considered necessary for the definition of 4Es ideas, is not however a sufficient condition to avoid the "classical sandwich". Finally, we argue that an emerging framework in the cognitive and brain sciences, active inference, extends ideas derived from control theory to the study of biological systems while disposing of the separation principle, describing non-modular models of behaviour strongly aligned with 4Es theories of cognition.},
archivePrefix = {arXiv},
arxivId = {1806.02649},
author = {Baltieri, Manuel and Buckley, Christopher L.},
eprint = {1806.02649},
file = {::},
month = {jun},
title = {{The modularity of action and perception revisited using control theory and active inference}},
url = {http://arxiv.org/abs/1806.02649},
year = {2018}
}
@article{Guerguiev2017,
abstract = {{\textless}p{\textgreater}Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations—the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.{\textless}/p{\textgreater}},
author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
doi = {10.7554/eLife.22901},
issn = {2050-084X},
journal = {eLife},
month = {dec},
title = {{Towards deep learning with segregated dendrites}},
url = {https://elifesciences.org/articles/22901},
volume = {6},
year = {2017}
}
@article{Singer2018,
abstract = {{\textless}p{\textgreater}Neurons in sensory cortex are tuned to diverse features in natural scenes. But what determines which features neurons become selective to? Here we explore the idea that neuronal selectivity is optimized to represent features in the recent sensory past that best predict immediate future inputs. We tested this hypothesis using simple feedforward neural networks, which were trained to predict the next few moments of video or audio in clips of natural scenes. The networks developed receptive fields that closely matched those of real cortical neurons in different mammalian species, including the oriented spatial tuning of primary visual cortex, the frequency selectivity of primary auditory cortex and, most notably, their temporal tuning properties. Furthermore, the better a network predicted future inputs the more closely its receptive fields resembled those in the brain. This suggests that sensory processing is optimized to extract those features with the most capacity to predict future input.{\textless}/p{\textgreater}},
author = {Singer, Yosef and Teramoto, Yayoi and Willmore, Ben DB and Schnupp, Jan WH and King, Andrew J and Harper, Nicol S},
doi = {10.7554/eLife.31557},
issn = {2050-084X},
journal = {eLife},
month = {jun},
title = {{Sensory cortex is optimized for prediction of future input}},
url = {https://elifesciences.org/articles/31557},
volume = {7},
year = {2018}
}
@article{Rohenkohl2018,
abstract = {Motor behavior is often driven by visual stimuli, relying on efficient feedforward communication from lower to higher visual areas. The Communication-through-Coherence hypothesis proposes that interareal communication depends on coherence at an optimal phase relation. While previous studies have linked effective communication to enhanced interareal coherence, it remains unclear, whether this interareal coherence occurs at an optimal phase relation that actually improves the stimulus transmission to behavioral report. We recorded local field potentials simultaneously from areas V1 and V4 of macaque monkeys performing a selective visual attention task, during which they reported changes of the attended stimulus. Gamma synchronization between V1 and V4, immediately preceding the stimulus change, predicted subsequent reaction times (RTs). Crucially, RTs were systematically slowed as trial-by-trial interareal gamma phase relations deviated from the phase relation at which V1 and V4 synchronized on average. These effects were specific to the attended stimulus and not due to local power or phase inside V1 or V4. We conclude that interareal gamma synchronization occurs at the optimal phase relation and thereby improves interareal communication and the effective transformation of sensory inputs into motor responses.},
author = {Rohenkohl, Gustavo and Bosman, Conrado A and Fries, Pascal},
file = {:home/sflippl/Documents/Literature/Papers/Rohenkohl, Bosman, Fries - 2018 - Gamma synchronization between V1 and V4 improves behavioral performance.pdf:pdf},
journal = {bioRxiv},
month = {jan},
title = {{Gamma synchronization between V1 and V4 improves behavioral performance}},
url = {http://biorxiv.org/content/early/2018/04/02/290817.abstract},
year = {2018}
}
@article{Marques2018,
abstract = {Cortical feedback is thought to mediate cognitive processes like attention, prediction, and awareness. Understanding its function requires identifying the organizational logic of feedback axons relaying different signals. We measured retinotopic specificity in inputs from the lateromedial visual area in mouse primary visual cortex (V1) by mapping receptive fields in feedback boutons and relating them to those of neurons in their vicinity. Lateromedial visual area inputs in layer 1 targeted, on average, retinotopically matched locations in V1, but many of them relayed distal visual information. Orientation-selective axons overspread around the retinotopically matched location perpendicularly to their preferred orientation. Direction-selective axons were biased to visual areas shifted from the retinotopically matched position along the angle of their antipreferred direction. Our results show that feedback inputs show tuning-dependent retinotopic specificity. By targeting locations that would be activated by stimuli orthogonal to or opposite to a cell's own tuning, feedback could potentially enhance visual representations in time and space.},
author = {Marques, Tiago and Nguyen, Julia and Fioreze, Gabriela and Petreanu, Leopoldo},
doi = {10.1038/s41593-018-0135-z},
file = {::},
issn = {1097-6256},
journal = {Nature Neuroscience},
keywords = {Neuroscience,Visual system},
month = {may},
number = {5},
pages = {757--764},
publisher = {Nature Publishing Group},
title = {{The functional organization of cortical feedback inputs to primary visual cortex}},
url = {http://www.nature.com/articles/s41593-018-0135-z},
volume = {21},
year = {2018}
}
@article{Rao1999,
abstract = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
author = {Rao, Rajesh P. N. and Ballard, Dana H.},
doi = {10.1038/4580},
file = {:home/sflippl/Documents/Literature/Papers/Rao, Ballard - 1999 - Predictive coding in the visual cortex a functional interpretation of some extra-classical receptive-field effects.pdf:pdf},
issn = {1097-6256},
journal = {Nature Neuroscience},
month = {jan},
number = {1},
pages = {79--87},
publisher = {Nature Publishing Group},
title = {{Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects}},
url = {http://www.nature.com/articles/nn0199{\_}79},
volume = {2},
year = {1999}
}
@article{Hassabis2017,
author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
doi = {10.1016/j.neuron.2017.06.011},
issn = {0896-6273},
journal = {Neuron},
number = {2},
pages = {245--258},
publisher = {Elsevier Inc.},
title = {{Neuroscience-Inspired Artificial Intelligence}},
url = {http://dx.doi.org/10.1016/j.neuron.2017.06.011},
volume = {95},
year = {2017}
}
@article{Dempster1977a,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
archivePrefix = {arXiv},
arxivId = {0710.5696v2},
author = {Dempster, A.P. and Laird, N.M. and Rubin, Donald B},
doi = {http://dx.doi.org/10.2307/2984875},
eprint = {0710.5696v2},
isbn = {0000000779},
issn = {00359246},
journal = {Journal of the Royal Statistical Society Series B Methodological},
number = {1},
pages = {1--38},
pmid = {9501024},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@article{Marblestone2016,
abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
author = {Marblestone, Adam H and Wayne, Greg and Kording, Konrad P},
doi = {10.3389/fncom.2016.00094},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
number = {94},
pages = {1--41},
title = {{Toward an Integration of Deep Learning and Neuroscience}},
url = {https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
volume = {10},
year = {2016}
}
@techreport{McLachlan2004,
abstract = {The Expectation-Maximization (EM) algorithm is a broadly applicable approach to the iterative computation of maximum likelihood (ML) estimates, useful in a variety of incomplete-data problems. Maximum likelihood estimation and likelihood-based inference are of central importance in statistical theory and data analysis. Maximum likelihood estimation is a general-purpose method with attractive properties. It is the most-often used estimation technique in the frequentist framework; it is also relevant in the Bayesian framework (Chapter III.11). Often Bayesian solutions are justified with the help of likelihoods and maximum likelihood estimates (MLE), and Bayesian solutions are similar to penalized likelihood estimates. Maximum likelihood estimation is an ubiquitous technique and is used extensively in every area where statistical techniques are used.},
author = {McLachlan, Geoffrey J and Krishnan, Thriyambakam and Ng, See Ket},
keywords = {330},
number = {24},
title = {{The EM Algorithm}},
type = {Papers / Humboldt-Universit{\"{a}}t Berlin, Center for Applied Statistics and Economics (CASE)},
url = {http://hdl.handle.net/10419/22198},
year = {2004}
}
@article{Friston2005,
author = {Friston, Karl},
doi = {10.1098/rstb.2005.1622},
journal = {Philosophical Transactions of the Royal Society B},
keywords = {bayesian,cortical,generative models,hierarchical,inference,predictive coding},
pages = {815--836},
title = {{A theory of cortical responses}},
volume = {360},
year = {2005}
}
@article{Whittington2017,
abstract = {To efficiently learn from feedback, cortical networks need to update synaptic weights onmultiple levels of cortical hierarchy.Aneffective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithmwith local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Hereweshowthat a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple localHebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive codingmodel converges to that of the backprop- agation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.},
annote = {Topic: Simple Hebbian model approximates neural network error backpropagation.

Two classes of biological supervised learning: 
stochastic neurons and synapses receiving a global feedback signal (via a neuromodulator (long-term neurotransmitter)) - often does not approximate backpropagation and scales poorly.
Explicitly approximates backpropagation.

Predictive coding framework uses additional nodes that encode the difference between the activity and the predicted activity on a certain level.

In Artifical Neural Networks, node correction is obtained by a global error estimate that can be computed via backpropagation. The errors could as well be included as separate nodes.},
archivePrefix = {arXiv},
arxivId = {1706.02451},
author = {Whittington, James C. R. and Bogacz, Rafal},
doi = {10.1162/NECO},
eprint = {1706.02451},
isbn = {0899-7667},
issn = {1530888X},
journal = {Neural Computation},
pages = {1229--1262},
pmid = {25602775},
title = {{An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity}},
url = {http://arxiv.org/abs/1706.02451},
volume = {29},
year = {2017}
}
